{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 08 (Monday), AST 8581 / PHYS 8581 / CSCI 8581: Big Data in Astrophysics\n",
    "\n",
    "### Michael Coughlin <cough052@umn.edu>, Jie Ding <dingj@umn.edu>\n",
    "\n",
    "with contributions totally ripped off from Gautham Narayan (UIUC), Michael Steinbach (UMN), and Nico Adams (UMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where do we stand?\n",
    "\n",
    "Foundations of Data and Probability -> Statistical frameworks (Frequentist vs Bayesian) -> Estimating underlying distributions -> Analysis of Time series (periodicity) -> Analysis of Time series (variability) -> Analysis of Time series (stochastic processes) -> Gaussian Processes -> Decision Trees / Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Last Class: Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Class: Decision Trees / Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Machine Learning ?\n",
    "\n",
    "* The umbrella term \"machine learning\" describes methods for *automated data analysis*, developed by computer scientists and statisticians in response to the appearance of ever larger datasets.\n",
    "\n",
    "* What is actually being learned? With GPs, you **specified** the functional form for the correlation between observations in your training set. With ML, you specify a notion of how to measure the distance between observations, and it learns the correlation structure and builds a model, $M$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The goal of automation has led to an emphasis on non-parametric models (that adapt to dataset size and complexity), and a very uniform terminology that enables multiple models to be implemented and compared on an equal footing.\n",
    "\n",
    "* Machine learning can be divided into two types: *supervised* and *unsupervised.* (this is for future classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* Supervised learning is also known as *predictive* learning. Given *inputs* $X$, the goal is to construct a machine that can accurately predict a set of *outputs* $y$, usually so that _decisions_ can be made. \n",
    "\n",
    "\n",
    "* The \"supervision\" refers to the education of the machine, via a *training set* $D$ of input-output pairs that we provide. Prediction accuracy is then tested on *validation* and *test* sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* At the heart of the prediction machine is a *model* $M$ that can be *trained* to give accurate predictions.\n",
    "\n",
    "* Supervised learning is about making predictions by characterizing ${\\rm Pr}(y_k|x_k,D,M)$.\n",
    "\n",
    "* The outputs $y$ are said to be *response variables* - predictions of $y$ will be generated by our model. \n",
    "\n",
    "* The variables $y$ can be either *categorical* (\"labels\") or *nominal* (real numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* When the $y$ are numerical, the problem is a *regression* (\"how should we interpolate between these numerical values?\").\n",
    "\n",
    "<img src=\"figures/house_price_features.png\">\n",
    "\n",
    "<img src=\"figures/house_price_features_corr.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/ml_map.png\"></img>\n",
    "\n",
    "> The [`scikit-learn` algorithm cheatsheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html), as provided with the package documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Representations\n",
    "\n",
    "* Each input $x$ is said to have $P$ *features* (or *attributes*), and represents a *sample* (assumed to have been drawn from a sampling distribution). Each sample input $x$ is associated with an output $y$.\n",
    "\n",
    "\n",
    "* Our $N$ input *samples* are packaged into an $N \\times P$ *design matrix* $X$ (with $N$ rows and $P$ columns). We've used this term before in the context of regression and you saw an example of building one with the HBM on Cepheids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/ml_data_representation.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Typically a supervised learning model is \"trained\" on a subset of the data, and then its ability to make predictions about new data \"tested\" on the remainder.\n",
    "\n",
    "* Training involves \"fitting\" the model to the data, optimizing its parameters to minimize some \"loss function\" (or equivalently, maximize some defined \"score\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/ml_supervised_workflow.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/ml_train_test_split_matrix.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizing Model Prediction Accuracy\n",
    "\n",
    "* In supervised machine learning the goal is to make the most accurate predictions we can - which means neither over-fitting nor under-fitting the data \n",
    "\n",
    "* The \"mean squared error\" between the model predictions and the truth is a useful metric: minimizing MSE corresponds to minimizing the \"empirical risk,\" defined as the mean value loss function averaged over the available data samples, where the loss function is quadratic\n",
    "\n",
    "<img src=\"figures/overfitting_underfitting_cartoon.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree Methods\n",
    "\n",
    "The hierarchical application of decision boundaries lead to decision trees\n",
    "\n",
    "Tree structure:\n",
    "\n",
    "- top node contains the entire data set\n",
    "- at each branch the data are subdivided into **two** child nodes (this is the decision)\n",
    "- split is based on a predefined decision boundary (usually axis aligned)\n",
    "- splitting repeats, recursively, until we reach a predefined stopping criteria\n",
    "\n",
    "\n",
    "Application of the tree to classification is simple (a series of binary decisions). \n",
    "\n",
    "The fraction of points from the training set classified as one class or the other defines the class associated with the decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OK, so how to pick what decision to make? \n",
    "\n",
    "i.e. How do we choose the best attribute to divide the data set on\n",
    "\n",
    "You need a **metric** to measure how good your feature is at predicting the desired output\n",
    "\n",
    "`sklearn` gives you two options\n",
    "- Entropy\n",
    "    - and a related quantity, the Information Gain\n",
    "- Gini index\n",
    "\n",
    "We are performing top-down search through the space of possible decision trees, and optimizing this metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entropy:\n",
    "\n",
    "The entropy (we've seen this before):\n",
    "\\begin{equation}\n",
    "\\huge\n",
    "H(S)=\\sum_{i=1}^{c}-p_{i} \\log _{2} p_{i}\n",
    "\\end{equation}\n",
    "\n",
    "If you imagine a system with two states $S$ (+) and (-) then visually:\n",
    "\n",
    "<img src=\"entropy_twoclass.png\">\n",
    "\n",
    "States with low entropy having high *purity* - they're more homogenous \n",
    "\n",
    "If a state has an entropy that is greater than zero, you can keep trying to make decisions - i.e. split further\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy for (0, 14): 0.00\n",
      "Entropy for (1, 13): 0.37\n",
      "Entropy for (2, 12): 0.59\n",
      "Entropy for (3, 11): 0.75\n",
      "Entropy for (4, 10): 0.86\n",
      "Entropy for (5, 9): 0.94\n",
      "Entropy for (6, 8): 0.99\n",
      "Entropy for (7, 7): 1.00\n"
     ]
    }
   ],
   "source": [
    "# You can compute the entropy with scipy pretty easily, but practically you won't have to do this manually\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "pick1 = np.arange(0, 8, 1)\n",
    "for p1 in pick1:\n",
    "    p2 = 14 - p1\n",
    "    print(f'Entropy for {p1, p2}: {st.entropy([p1,p2], base=2):.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Information gain:\n",
    "\n",
    "in terms of the Entropy\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\huge\n",
    "I(\\text{parent}, \\text{child})=H(\\text{parent})-H(\\text{parent}|\\text{child})\n",
    "\\end{equation}\n",
    "\n",
    "The second term has two values since each parent node in the decision tree has two children - weight by fraction of the population at each child (default - there are other weighting schemes).\n",
    "\n",
    "\n",
    "You can measure the entropy with a log of any base, but 2 is conventional because in that case, the entropy is in units of *binary digits* or **bits.** \n",
    "\n",
    "This can be directly interpreted in the information processing sense as the number of bits required to represent the event. \n",
    "\n",
    "This quantity is also often called the **Kullback Leibler divergence** or just **KL divergence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "\\huge\n",
    "I(\\text{parent}, \\text{child})=H(\\text{parent})-H(\\text{parent}|\\text{child})\n",
    "\\end{equation}\n",
    "\n",
    "# <center> This is a distance </center>\n",
    "\n",
    "This is really what we are specifying with all ML algorithms\n",
    "\n",
    "Note the difference with what we did earlier\n",
    "* with parameteric methods, you defined the model in terms of parameters you knew were interesting\n",
    "    * and wrote down a likelihood which is a distance between your model and some of the observations (a training set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* With non-parametric methods, you didn't define a model, but you did define some basis (sines or Gaussians but many other options) \n",
    "    * expressed in terms of the variable you decided were interesting (time/position/whatever)\n",
    "    * and wrote down a form for the correlation between different values of that variable - the kernel\n",
    "    * you held out some of your observations as a training set \n",
    "    * which let you compute a covariance matrix given your kernel \n",
    "    * which let you write down a likelihood which is a distance between the conditioned model and the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Now you aren't even defining what the variables in the model are, but you are defining a measure of importance that can be used to weight each state in your model\n",
    "    * and a form for the distance between two states in terms of infromation that is gained or loss - a **loss function**\n",
    "    * and you hold out some of your observations as a training set\n",
    "    * and minimize the loss function to condition the model\n",
    "    * which in supervised learning is reducing the entropy at each node (i.e. making decisions such that the output is a more homogenous subset)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gini Impurity\n",
    "\n",
    "1. Randomly pick a datapoint in our dataset\n",
    "2. Randomly classify it according to the class distribution in the dataset\n",
    "    (i.e. if you 6 red, 2 green, 3 blue things in some dataset with 11 samples, then (6/11, 2/11 and 3/11 respectively)\n",
    "    \n",
    "### Whatâ€™s the probability we classify the datapoint (in)correctly? \n",
    "\n",
    "The answer to that question is the Gini (Im)purity.\n",
    "\n",
    "\\begin{equation}\n",
    "\\huge\n",
    "G(S)=\\sum_{i=1}^{C} p(i) \\cdot(1-p(i))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And the Gini gain\n",
    "\n",
    "\\begin{equation}\n",
    "\\huge\n",
    "I(\\text{parent}, \\text{child})=G(\\text{parent})-G(\\text{parent}|\\text{child})\n",
    "\\end{equation}\n",
    "\n",
    "Yup. This is a distance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-class warm-up: Look at the table below and pick the feature that is best to split on first\n",
    "\n",
    "\n",
    "#### Remember that you only get to make a binary split (i.e. mild or not mild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>Table length=14</i>\n",
       "<table id=\"table140240401910032\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>outlook</th><th>temperature</th><th>humidity</th><th>windy</th><th>play</th></tr></thead>\n",
       "<thead><tr><th>str8</th><th>str4</th><th>str6</th><th>str5</th><th>str3</th></tr></thead>\n",
       "<tr><td>sunny</td><td>hot</td><td>high</td><td>FALSE</td><td>no</td></tr>\n",
       "<tr><td>sunny</td><td>hot</td><td>high</td><td>TRUE</td><td>no</td></tr>\n",
       "<tr><td>overcast</td><td>hot</td><td>high</td><td>FALSE</td><td>yes</td></tr>\n",
       "<tr><td>rainy</td><td>mild</td><td>high</td><td>FALSE</td><td>yes</td></tr>\n",
       "<tr><td>rainy</td><td>cool</td><td>normal</td><td>FALSE</td><td>yes</td></tr>\n",
       "<tr><td>rainy</td><td>cool</td><td>normal</td><td>TRUE</td><td>no</td></tr>\n",
       "<tr><td>overcast</td><td>cool</td><td>normal</td><td>TRUE</td><td>yes</td></tr>\n",
       "<tr><td>sunny</td><td>mild</td><td>high</td><td>FALSE</td><td>no</td></tr>\n",
       "<tr><td>sunny</td><td>cool</td><td>normal</td><td>FALSE</td><td>yes</td></tr>\n",
       "<tr><td>rainy</td><td>mild</td><td>normal</td><td>FALSE</td><td>yes</td></tr>\n",
       "<tr><td>sunny</td><td>mild</td><td>normal</td><td>TRUE</td><td>yes</td></tr>\n",
       "<tr><td>overcast</td><td>mild</td><td>high</td><td>TRUE</td><td>yes</td></tr>\n",
       "<tr><td>overcast</td><td>hot</td><td>normal</td><td>FALSE</td><td>yes</td></tr>\n",
       "<tr><td>rainy</td><td>mild</td><td>high</td><td>TRUE</td><td>no</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table length=14>\n",
       "outlook  temperature humidity windy play\n",
       "  str8       str4      str6    str5 str3\n",
       "-------- ----------- -------- ----- ----\n",
       "   sunny         hot     high FALSE   no\n",
       "   sunny         hot     high  TRUE   no\n",
       "overcast         hot     high FALSE  yes\n",
       "   rainy        mild     high FALSE  yes\n",
       "   rainy        cool   normal FALSE  yes\n",
       "   rainy        cool   normal  TRUE   no\n",
       "overcast        cool   normal  TRUE  yes\n",
       "   sunny        mild     high FALSE   no\n",
       "   sunny        cool   normal FALSE  yes\n",
       "   rainy        mild   normal FALSE  yes\n",
       "   sunny        mild   normal  TRUE  yes\n",
       "overcast        mild     high  TRUE  yes\n",
       "overcast         hot   normal FALSE  yes\n",
       "   rainy        mild     high  TRUE   no"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS\n",
    "import astropy.table as at\n",
    "import graphviz \n",
    "weather = at.Table.read('data/weather_nominal.csv', format='ascii')\n",
    "weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementing a Single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlook</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windy</th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    outlook  temperature  humidity  windy  play\n",
       "0       2.0          1.0       0.0    0.0   0.0\n",
       "1       2.0          1.0       0.0    1.0   0.0\n",
       "2       0.0          1.0       0.0    0.0   1.0\n",
       "3       1.0          2.0       0.0    0.0   1.0\n",
       "4       1.0          0.0       1.0    0.0   1.0\n",
       "5       1.0          0.0       1.0    1.0   0.0\n",
       "6       0.0          0.0       1.0    1.0   1.0\n",
       "7       2.0          2.0       0.0    0.0   0.0\n",
       "8       2.0          0.0       1.0    0.0   1.0\n",
       "9       1.0          2.0       1.0    0.0   1.0\n",
       "10      2.0          2.0       1.0    1.0   1.0\n",
       "11      0.0          2.0       0.0    1.0   1.0\n",
       "12      0.0          1.0       1.0    0.0   1.0\n",
       "13      1.0          2.0       0.0    1.0   0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# convert the labels to numerical data\n",
    "cols = []\n",
    "new_weather = weather.copy()\n",
    "for i, feature in enumerate(weather.colnames):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    new_weather[feature] = le.fit_transform(weather[feature])*1.\n",
    "new_weather = new_weather.to_pandas()\n",
    "new_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"287pt\" height=\"269pt\"\n",
       " viewBox=\"0.00 0.00 287.00 269.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 265)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-265 283,-265 283,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"155,-261 55,-261 55,-193 155,-193 155,-261\"/>\n",
       "<text text-anchor=\"middle\" x=\"105\" y=\"-245.8\" font-family=\"Times,serif\" font-size=\"14.00\">outlook &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"105\" y=\"-230.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.94</text>\n",
       "<text text-anchor=\"middle\" x=\"105\" y=\"-215.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 14</text>\n",
       "<text text-anchor=\"middle\" x=\"105\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [9, 5]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"92,-149.5 0,-149.5 0,-96.5 92,-96.5 92,-149.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-134.3\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 4</text>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-104.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = [4, 0]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M85.84,-192.88C79.42,-181.78 72.24,-169.37 65.77,-158.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"68.8,-156.42 60.76,-149.52 62.74,-159.93 68.8,-156.42\"/>\n",
       "<text text-anchor=\"middle\" x=\"54.31\" y=\"-169.97\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"217.5,-157 110.5,-157 110.5,-89 217.5,-89 217.5,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"164\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\">humidity &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"164\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 1.0</text>\n",
       "<text text-anchor=\"middle\" x=\"164\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 10</text>\n",
       "<text text-anchor=\"middle\" x=\"164\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [5, 5]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M124.16,-192.88C129.1,-184.33 134.49,-175.01 139.66,-166.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"142.76,-167.71 144.74,-157.3 136.7,-164.2 142.76,-167.71\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.19\" y=\"-177.75\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"155,-53 49,-53 49,0 155,0 155,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"102\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.722</text>\n",
       "<text text-anchor=\"middle\" x=\"102\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 5</text>\n",
       "<text text-anchor=\"middle\" x=\"102\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [1, 4]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M142.32,-88.95C136.55,-80.17 130.32,-70.66 124.51,-61.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"127.3,-59.68 118.89,-53.24 121.44,-63.52 127.3,-59.68\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"279,-53 173,-53 173,0 279,0 279,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"226\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.722</text>\n",
       "<text text-anchor=\"middle\" x=\"226\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 5</text>\n",
       "<text text-anchor=\"middle\" x=\"226\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [4, 1]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M185.68,-88.95C191.45,-80.17 197.68,-70.66 203.49,-61.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"206.56,-63.52 209.11,-53.24 200.7,-59.68 206.56,-63.52\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f8c4356c9d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the training features and target\n",
    "X_train = new_weather[weather.colnames[0:4]]\n",
    "Y_train = 1.-new_weather['play']\n",
    "\n",
    "# TWO LINES OF CODE TO IMPLEMENT A DECISION TREE\n",
    "\n",
    "# build the decision tree\n",
    "#clf = DecisionTreeClassifier(...\n",
    "\n",
    "# Step 3: Train the model on the data\n",
    "#clf.fit(...\n",
    "\n",
    "# Plot the Tree\n",
    "dot_data = tree.export_graphviz(clf, feature_names= weather.colnames[0:4], out_file=None) \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Decision trees are simple to interpret (a set of questions).\n",
    "\n",
    "[This structure is called a Dendrogram](https://en.wikipedia.org/wiki/Dendrogram)\n",
    "\n",
    "<img src=\"figures/tree_components.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remember the bias-variance tradeoff?\n",
    "<img src=\"figures/overfitting_underfitting_cartoon.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# There's not a lot you can do with a tree structure:\n",
    "<img src=\"figures/tree_depth.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Limiting Model Complexity\n",
    "<img src=\"figures/tree_limit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pruning - getting rid of leaves that don't have large information/gini gain\n",
    "<img src=\"figures/tree_pruning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-class warm-up: Predicting Survival from the Titanic Sinking\n",
    "\n",
    "This is the \"Hello World\" of Kaggle challenges.\n",
    "\n",
    "<img src=\"figures/titanic-sinking-hoyt.jpg\">\n",
    "\n",
    "We'll read in the file `titanic_train.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's a description of the columns\n",
    "<img src=\"figures/titanic_data.png\">\n",
    "\n",
    "Pick columns (i.e. features) that you think might have an impact on survival\n",
    "\n",
    "**CAUTION**: Some of these features may be missing for some samples - i.e. there is missing data for some people - if you use astropy (> 4.0) it will construct a table with masked columns. If you use an older astropy or pandas, you will need to modify your code accordingly. Irrespective, drop samples with NaN values. There should be 714 passengers after that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## BUILD A DECISION TREE AND DETERMINE WHAT VARIABLES ARE IMPORTANT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from astropy.visualization import hist\n",
    "import sklearn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>Table length=714</i>\n",
       "<table id=\"table140240401760080\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>PassengerId</th><th>Survived</th><th>Pclass</th><th>Name</th><th>Sex</th><th>Age</th><th>SibSp</th><th>Parch</th><th>Ticket</th><th>Fare</th><th>Cabin</th><th>Embarked</th></tr></thead>\n",
       "<thead><tr><th>int64</th><th>int64</th><th>int64</th><th>str82</th><th>str6</th><th>float64</th><th>int64</th><th>int64</th><th>str18</th><th>float64</th><th>str15</th><th>str1</th></tr></thead>\n",
       "<tr><td>1</td><td>0</td><td>3</td><td>Braund, Mr. Owen Harris</td><td>male</td><td>22.0</td><td>1</td><td>0</td><td>A/5 21171</td><td>7.25</td><td>--</td><td>S</td></tr>\n",
       "<tr><td>2</td><td>1</td><td>1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td>38.0</td><td>1</td><td>0</td><td>PC 17599</td><td>71.2833</td><td>C85</td><td>C</td></tr>\n",
       "<tr><td>3</td><td>1</td><td>3</td><td>Heikkinen, Miss. Laina</td><td>female</td><td>26.0</td><td>0</td><td>0</td><td>STON/O2. 3101282</td><td>7.925</td><td>--</td><td>S</td></tr>\n",
       "<tr><td>4</td><td>1</td><td>1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td><td>female</td><td>35.0</td><td>1</td><td>0</td><td>113803</td><td>53.1</td><td>C123</td><td>S</td></tr>\n",
       "<tr><td>5</td><td>0</td><td>3</td><td>Allen, Mr. William Henry</td><td>male</td><td>35.0</td><td>0</td><td>0</td><td>373450</td><td>8.05</td><td>--</td><td>S</td></tr>\n",
       "<tr><td>7</td><td>0</td><td>1</td><td>McCarthy, Mr. Timothy J</td><td>male</td><td>54.0</td><td>0</td><td>0</td><td>17463</td><td>51.8625</td><td>E46</td><td>S</td></tr>\n",
       "<tr><td>8</td><td>0</td><td>3</td><td>Palsson, Master. Gosta Leonard</td><td>male</td><td>2.0</td><td>3</td><td>1</td><td>349909</td><td>21.075</td><td>--</td><td>S</td></tr>\n",
       "<tr><td>9</td><td>1</td><td>3</td><td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td><td>female</td><td>27.0</td><td>0</td><td>2</td><td>347742</td><td>11.1333</td><td>--</td><td>S</td></tr>\n",
       "<tr><td>10</td><td>1</td><td>2</td><td>Nasser, Mrs. Nicholas (Adele Achem)</td><td>female</td><td>14.0</td><td>1</td><td>0</td><td>237736</td><td>30.0708</td><td>--</td><td>C</td></tr>\n",
       "<tr><td>11</td><td>1</td><td>3</td><td>Sandstrom, Miss. Marguerite Rut</td><td>female</td><td>4.0</td><td>1</td><td>1</td><td>PP 9549</td><td>16.7</td><td>G6</td><td>S</td></tr>\n",
       "<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "<tr><td>881</td><td>1</td><td>2</td><td>Shelley, Mrs. William (Imanita Parrish Hall)</td><td>female</td><td>25.0</td><td>0</td><td>1</td><td>230433</td><td>26.0</td><td>--</td><td>S</td></tr>\n",
       "<tr><td>882</td><td>0</td><td>3</td><td>Markun, Mr. Johann</td><td>male</td><td>33.0</td><td>0</td><td>0</td><td>349257</td><td>7.8958</td><td>--</td><td>S</td></tr>\n",
       "<tr><td>883</td><td>0</td><td>3</td><td>Dahlberg, Miss. Gerda Ulrika</td><td>female</td><td>22.0</td><td>0</td><td>0</td><td>7552</td><td>10.5167</td><td>--</td><td>S</td></tr>\n",
       "<tr><td>884</td><td>0</td><td>2</td><td>Banfield, Mr. Frederick James</td><td>male</td><td>28.0</td><td>0</td><td>0</td><td>C.A./SOTON 34068</td><td>10.5</td><td>--</td><td>S</td></tr>\n",
       "<tr><td>885</td><td>0</td><td>3</td><td>Sutehall, Mr. Henry Jr</td><td>male</td><td>25.0</td><td>0</td><td>0</td><td>SOTON/OQ 392076</td><td>7.05</td><td>--</td><td>S</td></tr>\n",
       "<tr><td>886</td><td>0</td><td>3</td><td>Rice, Mrs. William (Margaret Norton)</td><td>female</td><td>39.0</td><td>0</td><td>5</td><td>382652</td><td>29.125</td><td>--</td><td>Q</td></tr>\n",
       "<tr><td>887</td><td>0</td><td>2</td><td>Montvila, Rev. Juozas</td><td>male</td><td>27.0</td><td>0</td><td>0</td><td>211536</td><td>13.0</td><td>--</td><td>S</td></tr>\n",
       "<tr><td>888</td><td>1</td><td>1</td><td>Graham, Miss. Margaret Edith</td><td>female</td><td>19.0</td><td>0</td><td>0</td><td>112053</td><td>30.0</td><td>B42</td><td>S</td></tr>\n",
       "<tr><td>890</td><td>1</td><td>1</td><td>Behr, Mr. Karl Howell</td><td>male</td><td>26.0</td><td>0</td><td>0</td><td>111369</td><td>30.0</td><td>C148</td><td>C</td></tr>\n",
       "<tr><td>891</td><td>0</td><td>3</td><td>Dooley, Mr. Patrick</td><td>male</td><td>32.0</td><td>0</td><td>0</td><td>370376</td><td>7.75</td><td>--</td><td>Q</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table length=714>\n",
       "PassengerId Survived Pclass ...   Fare  Cabin Embarked\n",
       "   int64     int64   int64  ... float64 str15   str1  \n",
       "----------- -------- ------ ... ------- ----- --------\n",
       "          1        0      3 ...    7.25    --        S\n",
       "          2        1      1 ... 71.2833   C85        C\n",
       "          3        1      3 ...   7.925    --        S\n",
       "          4        1      1 ...    53.1  C123        S\n",
       "          5        0      3 ...    8.05    --        S\n",
       "          7        0      1 ... 51.8625   E46        S\n",
       "          8        0      3 ...  21.075    --        S\n",
       "          9        1      3 ... 11.1333    --        S\n",
       "         10        1      2 ... 30.0708    --        C\n",
       "         11        1      3 ...    16.7    G6        S\n",
       "        ...      ...    ... ...     ...   ...      ...\n",
       "        881        1      2 ...    26.0    --        S\n",
       "        882        0      3 ...  7.8958    --        S\n",
       "        883        0      3 ... 10.5167    --        S\n",
       "        884        0      2 ...    10.5    --        S\n",
       "        885        0      3 ...    7.05    --        S\n",
       "        886        0      3 ...  29.125    --        Q\n",
       "        887        0      2 ...    13.0    --        S\n",
       "        888        1      1 ...    30.0   B42        S\n",
       "        890        1      1 ...    30.0  C148        C\n",
       "        891        0      3 ...    7.75    --        Q"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS\n",
    "\n",
    "# Read in the data\n",
    "train = at.Table.read('data/titanic_train.csv', format='ascii')\n",
    "\n",
    "# mask out rows that are NaNs\n",
    "train = train[~train['Age'].mask]\n",
    "\n",
    "# Look at the data\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "X_train = train['Pclass', 'Sex', 'Age']\n",
    "X_train['Pclass'] = np.array(X_train['Pclass']).astype('f8')\n",
    "X_train['Sex'] = [1. if s =='female' else 0. for s in train['Sex']]\n",
    "\n",
    "Y_train = np.array(train['Survived']).astype('f8')\n",
    "X_train = X_train.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"976pt\" height=\"373pt\"\n",
       " viewBox=\"0.00 0.00 976.00 373.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 369)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-369 972,-369 972,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"544,-365 424,-365 424,-297 544,-297 544,-365\"/>\n",
       "<text text-anchor=\"middle\" x=\"484\" y=\"-349.8\" font-family=\"Times,serif\" font-size=\"14.00\">Sex &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"484\" y=\"-334.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.974</text>\n",
       "<text text-anchor=\"middle\" x=\"484\" y=\"-319.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 714</text>\n",
       "<text text-anchor=\"middle\" x=\"484\" y=\"-304.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [424, 290]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"418.5,-261 305.5,-261 305.5,-193 418.5,-193 418.5,-261\"/>\n",
       "<text text-anchor=\"middle\" x=\"362\" y=\"-245.8\" font-family=\"Times,serif\" font-size=\"14.00\">Pclass &lt;= 1.5</text>\n",
       "<text text-anchor=\"middle\" x=\"362\" y=\"-230.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.732</text>\n",
       "<text text-anchor=\"middle\" x=\"362\" y=\"-215.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 453</text>\n",
       "<text text-anchor=\"middle\" x=\"362\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [360, 93]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M444.39,-296.88C433.31,-287.62 421.15,-277.45 409.66,-267.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"411.75,-265.03 401.83,-261.3 407.26,-270.4 411.75,-265.03\"/>\n",
       "<text text-anchor=\"middle\" x=\"404.05\" y=\"-282.5\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"664.5,-261 551.5,-261 551.5,-193 664.5,-193 664.5,-261\"/>\n",
       "<text text-anchor=\"middle\" x=\"608\" y=\"-245.8\" font-family=\"Times,serif\" font-size=\"14.00\">Pclass &lt;= 2.5</text>\n",
       "<text text-anchor=\"middle\" x=\"608\" y=\"-230.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.804</text>\n",
       "<text text-anchor=\"middle\" x=\"608\" y=\"-215.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 261</text>\n",
       "<text text-anchor=\"middle\" x=\"608\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [64, 197]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M524.26,-296.88C535.52,-287.62 547.88,-277.45 559.56,-267.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"562.02,-270.36 567.52,-261.3 557.57,-264.95 562.02,-270.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"565.1\" y=\"-282.48\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"230,-157 124,-157 124,-89 230,-89 230,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\">Age &lt;= 53.0</text>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.969</text>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 101</text>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [61, 40]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M305.49,-194.84C284.48,-183.26 260.53,-170.05 239.05,-158.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"240.45,-154.99 230.01,-153.22 237.07,-161.12 240.45,-154.99\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"418.5,-157 305.5,-157 305.5,-89 418.5,-89 418.5,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"362\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\">Age &lt;= 9.5</text>\n",
       "<text text-anchor=\"middle\" x=\"362\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.611</text>\n",
       "<text text-anchor=\"middle\" x=\"362\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 352</text>\n",
       "<text text-anchor=\"middle\" x=\"362\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [299, 53]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M362,-192.88C362,-184.78 362,-175.98 362,-167.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"365.5,-167.3 362,-157.3 358.5,-167.3 365.5,-167.3\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"106,-53 0,-53 0,0 106,0 106,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"53\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.997</text>\n",
       "<text text-anchor=\"middle\" x=\"53\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 79</text>\n",
       "<text text-anchor=\"middle\" x=\"53\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [42, 37]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M133.63,-88.95C121.03,-79.34 107.29,-68.87 94.79,-59.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"96.85,-56.51 86.77,-53.24 92.6,-62.08 96.85,-56.51\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"230,-53 124,-53 124,0 230,0 230,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.575</text>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 22</text>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [19, 3]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M177,-88.95C177,-80.72 177,-71.85 177,-63.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"180.5,-63.24 177,-53.24 173.5,-63.24 180.5,-63.24\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"354,-53 248,-53 248,0 354,0 354,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"301\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.987</text>\n",
       "<text text-anchor=\"middle\" x=\"301\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 30</text>\n",
       "<text text-anchor=\"middle\" x=\"301\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [13, 17]</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M340.67,-88.95C335,-80.17 328.86,-70.66 323.15,-61.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"325.98,-59.74 317.61,-53.24 320.1,-63.54 325.98,-59.74\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"485.5,-53 372.5,-53 372.5,0 485.5,0 485.5,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"429\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.505</text>\n",
       "<text text-anchor=\"middle\" x=\"429\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 322</text>\n",
       "<text text-anchor=\"middle\" x=\"429\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [286, 36]</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M385.43,-88.95C391.72,-80.07 398.54,-70.46 404.87,-61.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"407.82,-63.42 410.75,-53.24 402.11,-59.37 407.82,-63.42\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"661,-157 555,-157 555,-89 661,-89 661,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"608\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\">Age &lt;= 2.5</text>\n",
       "<text text-anchor=\"middle\" x=\"608\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.314</text>\n",
       "<text text-anchor=\"middle\" x=\"608\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 159</text>\n",
       "<text text-anchor=\"middle\" x=\"608\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [9, 150]</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M608,-192.88C608,-184.78 608,-175.98 608,-167.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"611.5,-167.3 608,-157.3 604.5,-167.3 611.5,-167.3\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"844,-157 738,-157 738,-89 844,-89 844,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"791\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\">Age &lt;= 38.5</text>\n",
       "<text text-anchor=\"middle\" x=\"791\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.996</text>\n",
       "<text text-anchor=\"middle\" x=\"791\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 102</text>\n",
       "<text text-anchor=\"middle\" x=\"791\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [55, 47]</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>8&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M664.9,-194.29C685.28,-182.93 708.35,-170.07 729.13,-158.49\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"730.85,-161.53 737.88,-153.61 727.44,-155.42 730.85,-161.53\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"596,-53 504,-53 504,0 596,0 596,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"550\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 1.0</text>\n",
       "<text text-anchor=\"middle\" x=\"550\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"550\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [1, 1]</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M587.71,-88.95C582.32,-80.17 576.49,-70.66 571.06,-61.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"574.01,-59.93 565.8,-53.24 568.04,-63.59 574.01,-59.93\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"720,-53 614,-53 614,0 720,0 720,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"667\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.29</text>\n",
       "<text text-anchor=\"middle\" x=\"667\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 157</text>\n",
       "<text text-anchor=\"middle\" x=\"667\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [8, 149]</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M628.64,-88.95C634.12,-80.17 640.05,-70.66 645.58,-61.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"648.6,-63.57 650.93,-53.24 642.67,-59.87 648.6,-63.57\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"844,-53 738,-53 738,0 844,0 844,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"791\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 1.0</text>\n",
       "<text text-anchor=\"middle\" x=\"791\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 90</text>\n",
       "<text text-anchor=\"middle\" x=\"791\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [44, 46]</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M791,-88.95C791,-80.72 791,-71.85 791,-63.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"794.5,-63.24 791,-53.24 787.5,-63.24 794.5,-63.24\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"968,-53 862,-53 862,0 968,0 968,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"915\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">entropy = 0.414</text>\n",
       "<text text-anchor=\"middle\" x=\"915\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 12</text>\n",
       "<text text-anchor=\"middle\" x=\"915\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [11, 1]</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M834.37,-88.95C846.97,-79.34 860.71,-68.87 873.21,-59.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"875.4,-62.08 881.23,-53.24 871.15,-56.51 875.4,-62.08\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f8c44fe1050>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# TWO LINES OF CODE TO IMPLEMENT A DECISION TREE\n",
    "\n",
    "# build the decision tree\n",
    "#clf = DecisionTreeClassifier(...\n",
    "\n",
    "# Step 3: Train the model on the data\n",
    "#clf.fit(...\n",
    "\n",
    "# Plot the Tree\n",
    "#dot_data = tree.export_graphviz(...\n",
    "                                clf, feature_names= ['Pclass', 'Sex', 'Age'], out_file=None) \n",
    "#graph = graphviz.Source(...\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression with Trees: When the target variable is nominal \n",
    "\n",
    "<img src=\"figures/tree_regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But you say \n",
    "\n",
    "### AH BUT ENTROPY AND GINI IMPURITY ARE DEFINED AS MARGINALS OVER CATEGORICAL VARIABLES, WOE IS ME HOW COULD THESE DISTANCE METRICS EVER POSSIBLY WORK FOR VARIABLES THAT ARE CONTINUOUS \n",
    "\n",
    "(Well ok maybe you don't say exactly that...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/tree_regression_metric.png\">\n",
    "\n",
    "Hello old friends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The big issues with trees\n",
    "\n",
    "* Variance - different trees lead to different results\n",
    "    - intuitively if you have just two continuous variables, then calculating the split for every node even with a depth = 2 tree is of order $\\infty^2$\n",
    "    \n",
    "You can view each tree as a single path that you can take do get the desired outcome.\n",
    "\n",
    "There are many possible paths, so we do the thing we always do and marginalize over them.\n",
    "\n",
    "In other words, go from a single decision tree to a many decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two common ensemble methods that use decision trees:\n",
    "\n",
    "1. Random Forests\n",
    "    - Trees run in parallel, independent of each other\n",
    "    - Each tree uses a random subset of the observations and features (**bagging**)\n",
    "        - the number of features selected per split level is typically limited to the square root of the total number of features\n",
    "    - Class predicted by majority vote - what class do most trees think an observation belongs to or average in the case of regression\n",
    "\n",
    "\n",
    "\n",
    "2. Gradient Boosted Trees\n",
    "    - Trees run in series \n",
    "    - Each tree uses different weights for the features, updating the weights from the previous tree\n",
    "    - The last tree makes the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class warm-up: using Random Forests for regression using SDSS galaxies with known redshifts (the target) and magnitudes as features - i.e. a photo-z estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from astroML.datasets import fetch_sdss_specgals\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch and prepare the data\n",
    "data = fetch_sdss_specgals()\n",
    "\n",
    "# put magnitudes in a matrix\n",
    "mag = np.vstack([data['modelMag_%s' % f] for f in 'ugriz']).T\n",
    "z = data['z']\n",
    "\n",
    "# train on ~60,000 points\n",
    "mag_train = mag[::10]\n",
    "z_train = z[::10]\n",
    "\n",
    "# test on ~6,000 distinct points\n",
    "mag_test = mag[1::100]\n",
    "z_test = z[1::100]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the results\n",
    "#  This is a long computation, so we'll save the results to a pickle.\n",
    "@pickle_results('photoz_forest.pkl')\n",
    "def compute_photoz_forest(depth):\n",
    "    rms_test = np.zeros(len(depth))\n",
    "    rms_train = np.zeros(len(depth))\n",
    "    i_best = 0\n",
    "    z_fit_best = None\n",
    "\n",
    "    for i, d in enumerate(depth):\n",
    "        # YOUR CODE HERE\n",
    "        #clf = RandomForestRegressor(...\n",
    "\n",
    "        clf.fit(mag_train, z_train)\n",
    "\n",
    "        z_fit_train = clf.predict(mag_train)\n",
    "        z_fit = clf.predict(mag_test)\n",
    "        rms_train[i] = np.mean(np.sqrt((z_fit_train - z_train) ** 2))\n",
    "        rms_test[i] = np.mean(np.sqrt((z_fit - z_test) ** 2))\n",
    "\n",
    "        if rms_test[i] <= rms_test[i_best]:\n",
    "            i_best = i\n",
    "            z_fit_best = z_fit\n",
    "\n",
    "    return rms_test, rms_train, i_best, z_fit_best\n",
    "\n",
    "\n",
    "depth = np.arange(1, 20)\n",
    "rms_test, rms_train, i_best, z_fit_best = compute_photoz_forest(depth)\n",
    "best_depth = depth[i_best]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(wspace=0.25,\n",
    "                    left=0.1, right=0.95,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# left panel: plot cross-validation results\n",
    "ax = fig.add_subplot(121)\n",
    "# plot RMS test and train vs. depth\n",
    "#ax.plot(...\n",
    "#ax.plot(...\n",
    "ax.legend(loc=1, prop=dict(size=13))\n",
    "\n",
    "ax.set_xlabel('depth of tree')\n",
    "ax.set_ylabel('rms error')\n",
    "\n",
    "ax.set_xlim(0, 21)\n",
    "ax.set_ylim(0.009,  0.04)\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "# right panel: plot best fit\n",
    "ax = fig.add_subplot(122)\n",
    "# plot z_fit vs z_test\n",
    "#ax.scatter(...\n",
    "ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "ax.text(0.03, 0.97, \"depth = %i\\nrms = %.3f\" % (best_depth, rms_test[i_best]),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlabel(r'$\\rm z_{true}$', fontsize=16)\n",
    "ax.set_ylabel(r'$\\rm z_{fit}$', fontsize=16)\n",
    "\n",
    "ax.set_xlim(-0.02, 0.4001)\n",
    "ax.set_ylim(-0.02, 0.4001)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class exercise: using Gradient Boosting for regression on the same data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from astroML.datasets import fetch_sdss_specgals\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch and prepare the data\n",
    "data = fetch_sdss_specgals()\n",
    "\n",
    "# put magnitudes in a matrix\n",
    "mag = np.vstack([data['modelMag_%s' % f] for f in 'ugriz']).T\n",
    "z = data['z']\n",
    "\n",
    "# train on ~60,000 points\n",
    "mag_train = mag[::10]\n",
    "z_train = z[::10]\n",
    "\n",
    "# test on ~6,000 distinct points\n",
    "mag_test = mag[1::100]\n",
    "z_test = z[1::100]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the results\n",
    "#  This is a long computation, so we'll save the results to a pickle.\n",
    "@pickle_results('photoz_boosting.pkl')\n",
    "def compute_photoz_forest(N_boosts):\n",
    "    rms_test = np.zeros(len(N_boosts))\n",
    "    rms_train = np.zeros(len(N_boosts))\n",
    "    i_best = 0\n",
    "    z_fit_best = None\n",
    "\n",
    "    for i, Nb in enumerate(N_boosts):\n",
    "        # YOUR CODE HERE\n",
    "        #clf = GradientBoostingRegressor(...\n",
    "        clf.fit(mag_train, z_train)\n",
    "\n",
    "        z_fit_train = clf.predict(mag_train)\n",
    "        z_fit = clf.predict(mag_test)\n",
    "        rms_train[i] = np.mean(np.sqrt((z_fit_train - z_train) ** 2))\n",
    "        rms_test[i] = np.mean(np.sqrt((z_fit - z_test) ** 2))\n",
    "\n",
    "        if rms_test[i] <= rms_test[i_best]:\n",
    "            i_best = i\n",
    "            z_fit_best = z_fit\n",
    "\n",
    "    return rms_test, rms_train, i_best, z_fit_best\n",
    "\n",
    "N_boosts = (10, 100, 200, 300, 400, 500)\n",
    "rms_test, rms_train, i_best, z_fit_best = compute_photoz_forest(N_boosts)\n",
    "best_N = N_boosts[i_best]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.subplots_adjust(wspace=0.25,\n",
    "                    left=0.1, right=0.95,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# left panel: plot cross-validation results\n",
    "ax = fig.add_subplot(121)\n",
    "# plot RMS test and train vs. number of boosts\n",
    "#ax.plot(...\n",
    "#ax.plot(...\n",
    "ax.legend(loc=1, prop=dict(size=13))\n",
    "\n",
    "ax.set_xlabel('number of boosts')\n",
    "ax.set_ylabel('rms error')\n",
    "ax.set_xlim(0, 510)\n",
    "ax.set_ylim(0.009,  0.032)\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "ax.text(0.03, 0.03, \"Tree depth: 3\",\n",
    "        ha='left', va='bottom', transform=ax.transAxes)\n",
    "\n",
    "# right panel: plot best fit\n",
    "ax = fig.add_subplot(122)\n",
    "# plot z_fit vs z_test\n",
    "#ax.scatter(...\n",
    "ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "ax.text(0.03, 0.97, \"N = %i\\nrms = %.3f\" % (best_N, rms_test[i_best]),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlabel(r'$\\rm z_{true}$', fontsize=16)\n",
    "ax.set_ylabel(r'$\\rm z_{fit}$', fontsize=16)\n",
    "\n",
    "ax.set_xlim(-0.02, 0.4001)\n",
    "ax.set_ylim(-0.02, 0.4001)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* When the $y$ are categorical, the problem is one of *classification* (\"is this an image of a `dog`, or my `dinner`?\"). \n",
    "\n",
    "<img src=\"figures/dog_or_dinner.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "* Also known as *descriptive* learning. Here the goal is \"knowledge discovery\" - detection of patterns in a dataset, that can then be used in supervised/model-based analyses. \n",
    "\n",
    "\n",
    "* Unsupervised learning is about *density estimation* - characterizing ${\\rm Pr}(x|\\theta,H)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "* Examples of unsupervised learning activities include:\n",
    "\n",
    "  * Clustering analysis of the $x$.\n",
    "  * Dimensionality reduction: principal component analysis, independent component analysis, etc.\n",
    "  \n",
    "  \n",
    "<img src=\"figures/ul_stocks_all.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/ul_clusters.png\">\n",
    "\n",
    "Credit: Lorien Hayden (Cornell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/ul_sectors.png\">\n",
    "\n",
    "Credit: Lorien Hayden (Cornell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/ul_stocks.png\">\n",
    "\n",
    "Credit: Lorien Hayden (Cornell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/ul_portfolio.png\">\n",
    "\n",
    "Credit: Lorien Hayden (Cornell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## But of course individual members of a population can look wildly different, so clustering is nice, but we want our  regression and forecasting infrastructure as well. \n",
    "\n",
    "## Even these can be affected by unforseen circumstances though.\n",
    "\n",
    "<img src=\"figures/ul_stocks_after_data_breach.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/ul_vs_sl1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/ul_vs_sl2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/ul_vs_sl3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/ul_vs_sl4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/ul_vs_sl5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/ul_vs_sl6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/ul_vs_sl7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Models\n",
    "\n",
    "Examples of data-driven, non-parametric models for use in supervised learning include K-nearest neighbors, Support Vector Machines, Random Forest, Neural Networks, and many more. \n",
    "\n",
    "Many can be used for either classification or regression.\n",
    "\n",
    "All have a number of **hyper-parameters** that govern their overall behavior, that need to be determined for any given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "$\\;\\;\\;\\;\\;{\\rm MSE} = \\mathcal{E} \\left[ (\\hat{y} - y^{\\rm true})^2 \\right] = \\mathcal{E} \\left[ (\\hat{y} - \\bar{y} + \\bar{y} - y^{\\rm true})^2 \\right] = \\mathcal{E} \\left[ (\\hat{y} - \\bar{y})^2 \\right] + (\\bar{y} - y^{\\rm true})^2$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; = {\\rm var}(\\hat{y}) + {\\rm bias}^2(\\hat{y})$\n",
    "\n",
    "\n",
    "* In general, different models reach different balances between the variance and bias of their predictions\n",
    "\n",
    "* A particular choice of loss function leads to a corresponding minimized risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "* With a single training/test split, one can characterize the _prediction error_ using, for example, the MSE. \n",
    "\n",
    "* The model that minimizes the *generalized prediction error* can be found (approximately) with *cross validation*, in which we consider multiple training/test splits, and look at the _mean prediction error_ across all of these _\"folds.\"_\n",
    "\n",
    "* How we design the folds matters: we want each subset of the data to be a _fair sample_ of the whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/ml_grid_search_cross_validation.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another layer of cross validation is still needed, since we need to guard against over-fitting to this particular training set: we need to try all possible training sets.\n",
    "\n",
    "* Once we have the hyperparameters that optimize the generalized prediction error, we can then fix them at their optimal values and train on model on the entire data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveat Emptor\n",
    "\n",
    "* Machine learning algorithms are designed to make good use of big, complex datasets, where there are likely to be many more correlations and connections than we have thought of yet. \n",
    "\n",
    "\n",
    "* In this approach we assume that we will be able to make better predictions by using flexible, \"non-parametric\" methods that scale with the size of the dataset and allow new relationships to emerge empirically\n",
    "\n",
    "\n",
    "* Additional work needs to be done to extract a full Bayesian posterior PDF (or even frequentist confidence intervals) for the model parameters - which are typically not the focus of a machine learning analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble Methods: Bagging\n",
    "\n",
    "Run multiple different models on the same data and learn from the ensemble\n",
    "\n",
    "We can improve the performance of decisions trees (especially when there are many features) by **bagging** (Bootstrap AGGregation). This averages the predictive results of a series of bootstrap samples.\n",
    "\n",
    "For a sample of $N$ points in a training set, bagging generates $K$ equally sized bootstrap samples from which to estimate the function $f_i(x)$. The final estimator, defined by bagging, is then\n",
    "\n",
    "\\begin{equation}\n",
    "\\huge\n",
    "f(x)=\\frac{1}{K} \\sum_{i}^{K} f_{i}(x)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "(Yep, this is just take the average of all the individual methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble Methods: Boosting\n",
    "\n",
    "Boosting is an ensemble approach motivated by the idea that combining many weak classifiers can result in an improved classification. \n",
    "\n",
    "Boosting creates models to attempt to correct the errors of the ensemble so far \n",
    "**i.e. we reweight the data based on how incorrectly the data were classified in the previous iteration.**\n",
    "\n",
    "- if you have $N$ data points in your sample with features $x$, and target $y$\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "x_{i} \\in \\mathbb{R}^{d}, y_{i} \\in\\{-1,1\\}\n",
    "\\end{equation}\n",
    "**initalize the weights to $1/N$**\n",
    "\n",
    "- Run the classification $h$ (i.e. hypotheses) with a weak clasifier $t$ times and compute the weighted classification error for each classifier\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "\\epsilon = \\frac{\\sum_{i=1}^{N} w_{i} I\\left(y_{i} \\neq h_{j}\\left(x_{i}\\right)\\right)}{\\sum_{i=1}^{N} w_{i}}\n",
    "\\end{equation}\n",
    "\n",
    "where $I$ is the indicator variable (1 if $y_i$ matches the hypotheses, 0 if not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The weight for each weak classifier is related to the weighted classification error\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "\\theta_{t}=\\frac{1}{2} \\ln \\left(\\frac{1-\\epsilon_{t}}{\\epsilon_{t}}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "- each time reweight each sample $i$ based on the previous performance of the classifier $t$\n",
    "\n",
    "For any classifier with accuracy higher than 50%, the weight is positive. \n",
    "The more accurate the classifier, the larger the weight.\n",
    "For a classifer with less than 50% accuracy, the weight is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "\\large\n",
    "w_{t+1}=\\frac{w_{t} \\exp \\left[-\\theta_{t} y_{i} h_{t} \\right]}{Z_{t}}\n",
    "\\end{equation}\n",
    "        \n",
    "Where the denominator is the normalization constant to make the sum of the weights 1. \n",
    "\n",
    "If a misclassified case is from a positive weighted classifier, the â€œexpâ€ term in the numerator would be always larger than 1. \n",
    "\n",
    "**NOTE THAT A WEAK CLASSIFIER WITH A NEGATIVE WEIGHT STILL CONTRIBUTES**\n",
    "(you can be wrong, but if you are wrong consistently, then you are still useful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can view this as just iteratively minimizing the exponential loss function\n",
    "\n",
    "\\begin{equation}\n",
    "\\huge\n",
    "L(y, F(x))=E\\left(e^{-y F(x)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $E$ is just weighted expectation value.\n",
    "\n",
    "[I'll spare you the math](https://towardsdatascience.com/boosting-algorithm-adaboost-b6737a9ee60c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- At the end of this procedure we allow the classifiers a weighted vote on the final classification\n",
    "\n",
    "\\begin{equation}\n",
    "\\huge\n",
    "H(x)=\\operatorname{sign}\\left(\\sum_{t=1}^{T} \\theta_{t} h_{t}(x)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "    \n",
    "The most popular form of boosting is that of adaptive boosting (helpfully implemented in a package called **AdaBoost**)\n",
    "\n",
    "A fundamental limitation of the boosted decision tree is the computation time for large data sets (they rely on a chain of classifiers)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
