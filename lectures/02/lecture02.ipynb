{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': 1920, 'height': 1080, 'scroll': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1920,\n",
    "        'height': 1080,\n",
    "        'scroll': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 02 (Monday), AST 8581 / PHYS 8581 / CSCI 8581: Big Data in Astrophysics\n",
    "\n",
    "### Michael Coughlin <cough052@umn.edu>, Jie Ding <dingj@umn.edu>\n",
    "\n",
    "\n",
    "With contributions totally ripped off from Zjelko Ivezic and Mario Juric (UW), Gordon Richards (Drexel), Federica Bianco (U. Del), Maria Suveges (EPFL), Gautham Narayan (UIUC), Michael Steinbach (UMN), and Nico Adams (UMN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RECAP\n",
    "\n",
    "* You ran your first python notebook on google colab or your own system (and optionally worked through the python tutorial)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More help with git/os commands/python in the help/ directory\n",
    "\n",
    "<center> <img src =\"./figures/git.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We’re not focusing on git/python/os usage - pick it up as we go.\n",
    "* What the class is NOT\n",
    "   * A Statistics Class\n",
    "   * A Math Methods Class\n",
    "   * A Computer Science Class\n",
    "   * A Programming Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src =\"./figures/survey.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WHAT ASTRONOMERS CAN MEASURE\n",
    "* Angular separations\n",
    "* Time differences\n",
    "* Energy differences\n",
    "\n",
    "###  That’s it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## WHAT ASTRONOMERS CAN MEASURE\n",
    "* Astrometry (angular position on the sky) - arcseconds\n",
    "* Related definition: 1 parsec (pc) = distance at which a distance of 1 AU (i.e. Earth-Sun) subtends and angle of 1 arcsecond, i.e. 1 pc = 1 AU/tan(1”) ~ 31 trillion kilometers or 3.26 light years (ly)\n",
    "* Photometry (how bright something is)\n",
    "* Flux = photons (or energy in ergs)/sec/cm^2\n",
    "* (Apparent) Magnitude = -2.5 log10(Flux) + const\n",
    "* (Absolute) Magnitude = -2.5log10(Luminosity) + const = magnitude you’d measure if you could move the source to 10 pc\n",
    "* Light curves = photometry vs time\n",
    "* Evolution in source brightness either because of intrinsic (supernovae, AGN) or extrinsic (asteroids, eclipsing binaries)\n",
    "* Spectroscopy = Energy vs wavelength/frequency\n",
    "* Images/maps = Energy vs position on the sky (clustering, spatial correlation functions)\n",
    "* Proper Motion = Astrometry vs time (e.g. stars, satellite galaxies, asteroids…)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center> <img src =\"./figures/spectrum.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center> <img src =\"./figures/images.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Temporal & Spatial Variation\n",
    "\n",
    "<center> <img src =\"./figures/spatial_temporal.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spatial Variation\n",
    "\n",
    "<center> <img src =\"./figures/cmb.png\" width=\"800\"> <img src =\"./figures/cmb_spectrum.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualization\n",
    "\n",
    "* Dealing with spectra (1-D data):\n",
    "   * Programmatic: Use pandas to load the ascii spectrum and plot it\n",
    "* Dealing with images (2-D data):\n",
    "   * Programmatic: Use astropy.io.fits to load a .fits image\n",
    "   * Use ds9 to look at the images and adjust the scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Statistical inference is a logical framework with which to test our beliefs of a noisy world against data.\n",
    "### We formalize our beliefs in a probabilistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# AXIOMS OF PROBABILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SOME KEY IDEAS\n",
    "* All of the data we collect include some degree of randomness\n",
    "* Any conclusions we draw must therefore incorporate some notion of uncertainty\n",
    "* There is a a correct answer - the Universe as we know it exists after all.\n",
    "* Theory gives us a useful model for it. The challenging is evaluating how likely that model is given the data\n",
    "* Data are constants.\n",
    "* Even if they were randomly generated by the Universe, the data that we have already collected are fixed numbers.\n",
    "* We describe things we don't know with perfect precision as \"random\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RANDOM VARIABLES\n",
    "\n",
    "<center> <img src =\"./figures/cartoon_probability.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center> <img src =\"./figures/random_variables.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Random Variables\n",
    "\n",
    "* Discrete:\n",
    "   * Spectral type (G2V, KIII)\n",
    "   * Galaxy type, galaxy zoo\n",
    "* Continuous:\n",
    "   * magnitude, flux, colour, radial velocity, parallax/distance, temperature, elemental abundances, magnetic field, age, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Statistical inference is a logical framework with which to test our beliefs of a noisy world against data.\n",
    "\n",
    "## We formalize our beliefs in a probabilistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## THE MULTIVARIATE CASE\n",
    "\n",
    "<center> <img src =\"./figures/2D.png\" width=\"800\"> </center>\n",
    "\n",
    "<center> <img src =\"./figures/marginal.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bayes\n",
    "\n",
    "<center> <img src =\"./figures/bayes.png\" width=\"800\"> </center>\n",
    "\n",
    "* Posterior Likelihood: How probable is the hypothesis given the data we observed\n",
    "* Prior: How probable was the hypothesis before we observed anything\n",
    "* Likelihood: How probable is the data given the hypothesis is true\n",
    "* Evidence: How probable is the data over all possible hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## IN CLASS EXERCISE\n",
    "* Use the file: data/sdssj120650-20130310-total_mcmc.hdf5\n",
    "   * Use h5py to look at this data - h5py.File() to open, and then use the keys() method to find what elements are store\n",
    "   * You want “chain” and then “position”\n",
    "* Use numpy to get the stored data as an array\n",
    "* Use matplotlib to visualize this point cloud (CAREFUL)\n",
    "* Use pandas to convert the first two columns of the numpy array to a dataframe\n",
    "* Use seaborn’s jointplot to visualize this dataframe (try hex, or a kde with every 100th sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# MOMENTS AND DISTRIBUTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Flat / Tophat distribution\n",
    "\n",
    "<center> <img src =\"./figures/tophat.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Gaussian / Normal distribution\n",
    "\n",
    "<center> <img src =\"./figures/normal.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Poisson distribution\n",
    "\n",
    "<center> <img src =\"./figures/poisson.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Skewness, asymmetry\n",
    "\n",
    "<center> <img src =\"./figures/skewness.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So many distributions\n",
    "\n",
    "<center> <img src =\"./figures/distributions.png\" width=\"800\"> </center>\n",
    "\n",
    "* Lawrence M Leemis & Jacquelyn T McQueston (2008) “Univariate Distribution Relationships”, The American Statistician, 62:1, 45-53, DOI: 10.1198/000313008X270448"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quantile\n",
    "\n",
    "<center> <img src =\"./figures/quantile.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* So given a nice distribution function, you can calculate the mean, variance and moments ...\n",
    "* ... but you don’t usually have a nice distribution function given to you.\n",
    "* The distribution function is the thing you are trying to infer!\n",
    "* P(H|D)\n",
    "* The thing you have are the data - observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IN CLASS EXERCISE\n",
    "* Use the file: data/sdssj120650-20130310-total_mcmc.hdf5\n",
    "* Use h5py to look at this data - h5py.File() to open, and then use the keys() method to find what elements are stored - you want “chain” and then “position”\n",
    "* Use pandas to convert the first two columns of the numpy array to a dataframe (maybe you should make the above a function) and again plot every 100th point with a low alpha using matplotlib\n",
    "* Remember our goal is to infer a hypothesis from data i.e. P(H|D)\n",
    "* Estimate the means and standard deviations in both x and y by eye\n",
    "* Now use scipy.stats.multi_variate normal to construct a distribution object in python and overlay it with matplotlib (https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.multivariate_normal.html)\n",
    "* Finally, see how well your eyeball estimate matches astroML.stats.fit_bivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESTIMATORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data, samples\n",
    "\n",
    "* Usually we have observations, e.g. additive process, i.e. y(i) = f(i) +  n(i), where f(i) is deterministic and n(i) is a random variable\n",
    "* We want a characterisation of the deterministic and random parts\n",
    "* Suppose something about the random variable, often normality\n",
    "* We assume models and then estimate the parameters of a distribution, moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile\n",
    "\n",
    "<center> <img src =\"./figures/qqplot.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN CLASS EXERCISE\n",
    "* Now that you know how to generate points from a distribution, we can use the QQ plot to compare distributions to each other, or to a normal distribution\n",
    "* Use scipy.stats to generate some random numbers from a normal, uniform, and Cauchy distribution\n",
    "* Use statsmodels.api.qqplot to produce a qq plot of these distributions\n",
    "* Now generate random numbers from two different normal distributions (different locations and variances) and concatenate them\n",
    "* Again check the QQ plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
