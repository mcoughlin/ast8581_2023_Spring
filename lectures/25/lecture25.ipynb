{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqu_lZAIAdfF"
   },
   "source": [
    "# Week 14 (Monday), AST 8581 / PHYS 8581 / CSCI 8581 / STAT 8581: Big Data in Astrophysics\n",
    "\n",
    "### Michael Coughlin <cough052@umn.edu>, Jie Ding <dingj@umn.edu>\n",
    "\n",
    "With contributions derived from docs.python-guide.org and Cyrille Rossant (the IPython Interactive Computing and Visualization Cookbook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAKBcTLQAdfM"
   },
   "source": [
    "# Where do we stand?\n",
    "\n",
    "Foundations of Data and Probability -> Statistical frameworks (Frequentist vs Bayesian) -> Estimating underlying distributions -> Analysis of Time series (periodicity) -> Analysis of Time series (variability) -> Analysis of Time series (stochastic processes) -> Gaussian Processes -> Decision Trees / Regression -> Dimensionality Reduction -> Principle Component Analysis -> Clustering -> Density Estimation / Anomaly Detection -> Supervised Learning -> Deep Learning -> Introduction to Databases - SQL -> Introduction to Databases - NoSQL -> Introduction to Multiprocessing -> Introduction to GPUs -> Unit Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJrkN-HYuxx3"
   },
   "source": [
    "# Unit Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StFDlmu4J-67"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Untested code is broken code. Manual testing is essential to ensuring that our software works as expected and does not contain critical bugs. However, manual testing is severely limited because bugs may be introduced at any time in the code.\n",
    "\n",
    "Nowadays, automated testing is a standard practice in software engineering. In this lesson, we will briefly cover important aspects of automated testing: unit tests, test-driven development, test coverage, and continuous integration. Following these practices is fundamental in order to produce high-quality software.\n",
    "\n",
    "Python has a native unit testing module that you can readily use (unittest). Other third-party unit testing packages exist. In this recipe, we will use py.test. It is installed by default in Anaconda, but you can also install it manually with `conda install pytest`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "By definition, a unit test must focus on one specific functionality. All unit tests should be completely independent. Writing a program as a collection of well-tested, mostly decoupled units forces you to write modular code that is more easily maintainable.\n",
    "\n",
    "In a Python package, a test_xxx.py module should accompany every Python module named xxx.py. This testing module contains unit tests that test functionality implemented in the xxx.py module.\n",
    "\n",
    "For a working example of this, compare the function suite:\n",
    "https://github.com/skyportal/skyportal/tree/master/skyportal/handlers/api\n",
    "with the corresponding set of test functions:\n",
    "https://github.com/skyportal/skyportal/tree/master/skyportal/tests/api\n",
    "\n",
    "\n",
    "\n",
    "Sometimes, your module's functions require preliminary work to run (for example, setting up the environment, creating data files, or setting up a web server). The unit testing framework can handle this via fixtures. The state of the system environment should be exactly the same before and after a testing module runs. If your tests affect the file system, they should do so in a temporary directory that is automatically deleted at the end of the tests. Testing frameworks such as py.test provide convenient facilities for this use-case.\n",
    "\n",
    "For a working example of this, see e.g.\n",
    "https://github.com/skyportal/skyportal/blob/master/skyportal/tests/conftest.py\n",
    "\n",
    "\n",
    "Tests typically involve many assertions. With py.test, you can simply use the builtin assert keyword. Further convenient assertion functions are provided by NumPy (see http://docs.scipy.org/doc/numpy/reference/routines.testing.html). They are especially useful when working with arrays. For example, np.testing.assert_allclose(x, y) asserts that the x and y arrays are almost equal, up to a given precision that can be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Test?\n",
    "\n",
    "Writing a full testing suite takes time. It imposes strong (but good) constraints on your code's architecture. It is a real investment, but it is always profitable in the long run. Also, knowing that your project is backed by a full testing suite is a real load off your mind.\n",
    "\n",
    "* First, thinking about unit tests from the beginning forces you to think about a modular architecture. It is really difficult to write unit tests for a monolithic program full of interdependencies.\n",
    "\n",
    "* Second, unit tests make it easier for you to find and fix bugs. If a unit test fails after introducing a change in the program, isolating and reproducing the bugs becomes trivial.\n",
    "\n",
    "* Third, unit tests help you avoid regressions, that is, fixed bugs that silently reappear in a later version. When you discover a new bug, you should write a specific failing unit test for it. To fix it, make this test pass. Now, if the bug reappears later, this unit test will fail and you will immediately be able to address it.\n",
    "\n",
    "When you write a complex program based on interdependent APIs, having a good test coverage for one module means that you can safely rely on it in other modules, without worrying about its behavior not conforming to its specification.\n",
    "\n",
    "Unit tests are just one type of automated tests. Other important types of tests include integration tests (making sure that different parts of the program work together) and functional tests (testing typical use-cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General rules of testing\n",
    "\n",
    "* A testing unit should focus on one tiny bit of functionality and prove it correct.\n",
    "* Each test unit must be fully independent. Each test must be able to run alone, and also within the test suite, regardless of the order that they are called. The implication of this rule is that each test must be loaded with a fresh dataset and may have to do some cleanup afterwards. This is usually handled by setUp() and tearDown() methods.\n",
    "* Try hard to make tests that run fast. If one single test needs more than a few milliseconds to run, development will be slowed down or the tests will not be run as often as is desirable. In some cases, tests can’t be fast because they need a complex data structure to work on, and this data structure must be loaded every time the test runs. Keep these heavier tests in a separate test suite that is run by some scheduled task, and run all other tests as often as needed.\n",
    "* Learn your tools and learn how to run a single test or a test case. Then, when developing a function inside a module, run this function’s tests frequently, ideally automatically when you save the code.\n",
    "* Always run the full test suite before a coding session, and run it again after. This will give you more confidence that you did not break anything in the rest of the code.\n",
    "* It is a good idea to implement a hook that runs all tests before pushing code to a shared repository.\n",
    "* If you are in the middle of a development session and have to interrupt your work, it is a good idea to write a broken unit test about what you want to develop next. When coming back to work, you will have a pointer to where you were and get back on track faster.\n",
    "* The first step when you are debugging your code is to write a new test pinpointing the bug. While it is not always possible to do, those bug catching tests are among the most valuable pieces of code in your project.\n",
    "* Use long and descriptive names for testing functions. The style guide here is slightly different than that of running code, where short names are often preferred. The reason is testing functions are never called explicitly. square() or even sqr() is ok in running code, but in testing code you would have names such as test_square_of_number_2(), test_square_negative_number(). These function names are displayed when a test fails, and should be as descriptive as possible.\n",
    "* When something goes wrong or has to be changed, and if your code has a good set of tests, you or other maintainers will rely largely on the testing suite to fix the problem or modify a given behavior. Therefore the testing code will be read as much as or even more than the running code. A unit test whose purpose is unclear is not very helpful in this case.\n",
    "* Another use of the testing code is as an introduction to new developers. When someone will have to work on the code base, running and reading the related testing code is often the best thing that they can do to start. They will or should discover the hot spots, where most difficulties arise, and the corner cases. If they have to add some functionality, the first step should be to add a test to ensure that the new functionality is not already a working path that has not been plugged into the interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StFDlmu4J-67"
   },
   "source": [
    "## How to do it\n",
    "\n",
    "1.  Let's write in a first.py file a simple function that returns the first element of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting first.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile first.py\n",
    "def first(l):\n",
    "    return l[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  To test this function, we write another function, the unit test, that checks our first function using an example and an assertion:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to first.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a first.py\n",
    "\n",
    "# This is appended to the file.\n",
    "def test_first():\n",
    "    assert first([1, 2, 3]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def first(l):\r\n",
      "    return l[0]\r\n",
      "Overwriting first.py\r\n",
      "%%writefile -a first.py\r\n",
      "\r\n",
      "\r\n",
      "# This is appended to the file.\r\n",
      "def test_first():\r\n",
      "    assert first([1, 2, 3]) == 1\r\n"
     ]
    }
   ],
   "source": [
    "%cat first.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  To run the unit test, we use the pytest executable (the ! means that we're calling an external program from Jupyter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.9.7, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\r\n",
      "rootdir: /Users/mcoughlin/Code/Teaching/AST8581/ast8581_2022_Spring_workspace/lecture/25\r\n",
      "plugins: anyio-2.2.0, ligo.skymap-0.6.1\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "first.py \u001b[32m.\u001b[0m\u001b[32m                                                               [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest first.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  Our test passes! Let's add another example with an empty list. We want our function to return None in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting first.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile first.py\n",
    "def first(l):\n",
    "    return l[0]\n",
    "\n",
    "def test_first():\n",
    "    assert first([1, 2, 3]) == 1\n",
    "    assert first([]) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.7, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /Users/mcoughlin/Code/Teaching/AST8581/ast8581_2022_Spring_workspace/lecture/25\n",
      "plugins: anyio-2.2.0, ligo.skymap-0.6.1\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "first.py \u001b[31mF\u001b[0m\u001b[31m                                                               [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________________ test_first __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_first\u001b[39;49;00m():\n",
      "        \u001b[94massert\u001b[39;49;00m first([\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m]) == \u001b[94m1\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m first([]) \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mfirst.py\u001b[0m:6: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "l = []\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mfirst\u001b[39;49;00m(l):\n",
      ">       \u001b[94mreturn\u001b[39;49;00m l[\u001b[94m0\u001b[39;49;00m]\n",
      "\u001b[1m\u001b[31mE       IndexError: list index out of range\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mfirst.py\u001b[0m:2: IndexError\n",
      "=========================== short test summary info ============================\n",
      "FAILED first.py::test_first - IndexError: list index out of range\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.05s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest first.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.  This time, our test fails. Let's fix it by modifying the first() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting first.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile first.py\n",
    "def first(l):\n",
    "    return l[0] if l else None\n",
    "\n",
    "def test_first():\n",
    "    assert first([1, 2, 3]) == 1\n",
    "    assert first([]) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.9.7, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\r\n",
      "rootdir: /Users/mcoughlin/Code/Teaching/AST8581/ast8581_2022_Spring_workspace/lecture/25\r\n",
      "plugins: anyio-2.2.0, ligo.skymap-0.6.1\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "first.py \u001b[32m.\u001b[0m\u001b[32m                                                               [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest first.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test passes again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixtures are used when we want to run some code before every test method. So instead of repeating the same code in every test we define fixtures. Usually, fixtures are used to initialize database connections, pass the base , etc\n",
    "\n",
    "A method is marked as a Pytest fixture by marking with\n",
    "\n",
    "`@pytest.fixture`\n",
    "\n",
    "A test method can use a Pytest fixture by mentioning the fixture as an input parameter.\n",
    "\n",
    "Create a new file test_basic_fixture.py with following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_fixtures.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a test_fixtures.py\n",
    "\n",
    "import pytest\n",
    "@pytest.fixture\n",
    "def supply_AA_BB_CC():\n",
    "    aa=25\n",
    "    bb =35\n",
    "    cc=45\n",
    "    return [aa,bb,cc]\n",
    "\n",
    "def test_comparewithAA(supply_AA_BB_CC):\n",
    "    zz=35\n",
    "    assert supply_AA_BB_CC[0]==zz,\"aa and zz comparison failed\"\n",
    "\n",
    "def test_comparewithBB(supply_AA_BB_CC):\n",
    "    zz=35\n",
    "    assert supply_AA_BB_CC[1]==zz,\"bb and zz comparison failed\"\n",
    "\n",
    "def test_comparewithCC(supply_AA_BB_CC):\n",
    "    zz=35\n",
    "    assert supply_AA_BB_CC[2]==zz,\"cc and zz comparison failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have a fixture named supply_AA_BB_CC. This method will return a list of 3 values.\n",
    "We have 3 test methods comparing against each of the values.\n",
    "Each of the test function has an input argument whose name is matching with an available fixture. Pytest then invokes the corresponding fixture method and the returned values will be stored in the input argument , here the list [25,35,45]. Now the list items are being used in test methods for the comparison.\n",
    "\n",
    "Now run the test and see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.7, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /Users/mcoughlin/Code/Teaching/AST8581/ast8581_2022_Spring_workspace/lecture/25\n",
      "plugins: anyio-2.2.0, ligo.skymap-0.6.1\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "test_fixtures.py \u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                     [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_comparewithAA ______________________________\u001b[0m\n",
      "\n",
      "supply_AA_BB_CC = [25, 35, 45]\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_comparewithAA\u001b[39;49;00m(supply_AA_BB_CC):\n",
      "        zz=\u001b[94m35\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m supply_AA_BB_CC[\u001b[94m0\u001b[39;49;00m]==zz,\u001b[33m\"\u001b[39;49;00m\u001b[33maa and zz comparison failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: aa and zz comparison failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 25 == 35\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_fixtures.py\u001b[0m:12: AssertionError\n",
      "\u001b[31m\u001b[1m______________________________ test_comparewithCC ______________________________\u001b[0m\n",
      "\n",
      "supply_AA_BB_CC = [25, 35, 45]\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_comparewithCC\u001b[39;49;00m(supply_AA_BB_CC):\n",
      "        zz=\u001b[94m35\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m supply_AA_BB_CC[\u001b[94m2\u001b[39;49;00m]==zz,\u001b[33m\"\u001b[39;49;00m\u001b[33mcc and zz comparison failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: cc and zz comparison failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 45 == 35\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_fixtures.py\u001b[0m:20: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_fixtures.py::test_comparewithAA - AssertionError: aa and zz compa...\n",
      "FAILED test_fixtures.py::test_comparewithCC - AssertionError: cc and zz compa...\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.05s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test test_fixtures.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test test_comparewithBB is passed since zz=BB=35, and the remaining 2 tests are failed.\n",
    "\n",
    "The fixture method has a scope only within that test file it is defined. If we try to access the fixture in some other test file , we will get an error saying fixture ‘supply_AA_BB_CC’ not found for the test methods in other files.\n",
    "\n",
    "To use the same fixture against multiple test files, we will create fixture methods in a file called conftest.py.\n",
    "\n",
    "Let’s see this by the below PyTest example. Create 3 files conftest.py, test_basic_fixture.py, test_basic_fixture2.py with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conftest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a conftest.py\n",
    "\n",
    "import pytest\n",
    "@pytest.fixture\n",
    "def supply_AA_BB_CC():\n",
    "    aa=25\n",
    "    bb =35\n",
    "    cc=45\n",
    "    return [aa,bb,cc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_basic_fixture.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a test_basic_fixture.py\n",
    "\n",
    "import pytest\n",
    "def test_comparewithAA(supply_AA_BB_CC):\n",
    "    zz=35\n",
    "    assert supply_AA_BB_CC[0]==zz,\"aa and zz comparison failed\"\n",
    "\n",
    "def test_comparewithBB(supply_AA_BB_CC):\n",
    "    zz=35\n",
    "    assert supply_AA_BB_CC[1]==zz,\"bb and zz comparison failed\"\n",
    "\n",
    "def test_comparewithCC(supply_AA_BB_CC):\n",
    "    zz=35\n",
    "    assert supply_AA_BB_CC[2]==zz,\"cc and zz comparison failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_basic_fixture2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a test_basic_fixture2.py\n",
    "\n",
    "import pytest\n",
    "def test_comparewithAA_file2(supply_AA_BB_CC):\n",
    "\tzz=25\n",
    "\tassert supply_AA_BB_CC[0]==zz,\"aa and zz comparison failed\"\n",
    "\n",
    "def test_comparewithBB_file2(supply_AA_BB_CC):\n",
    "\tzz=25\n",
    "\tassert supply_AA_BB_CC[1]==zz,\"bb and zz comparison failed\"\n",
    "\n",
    "def test_comparewithCC_file2(supply_AA_BB_CC):\n",
    "\tzz=25\n",
    "\tassert supply_AA_BB_CC[2]==zz,\"cc and zz comparison failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest will look for the fixture in the test file first and if not found it will look in the conftest.py\n",
    "\n",
    "Run the test by py.test -k test_comparewith -v:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.7, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /Users/mcoughlin/opt/anaconda3/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/mcoughlin/Code/Teaching/AST8581/ast8581_2022_Spring_workspace/lecture/25\n",
      "plugins: anyio-2.2.0, ligo.skymap-0.6.1\n",
      "collected 9 items                                                              \u001b[0m\n",
      "\n",
      "test_basic_fixture.py::test_comparewithAA \u001b[31mFAILED\u001b[0m\u001b[31m                         [ 11%]\u001b[0m\n",
      "test_basic_fixture.py::test_comparewithBB \u001b[32mPASSED\u001b[0m\u001b[31m                         [ 22%]\u001b[0m\n",
      "test_basic_fixture.py::test_comparewithCC \u001b[31mFAILED\u001b[0m\u001b[31m                         [ 33%]\u001b[0m\n",
      "test_basic_fixture2.py::test_comparewithAA_file2 \u001b[32mPASSED\u001b[0m\u001b[31m                  [ 44%]\u001b[0m\n",
      "test_basic_fixture2.py::test_comparewithBB_file2 \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 55%]\u001b[0m\n",
      "test_basic_fixture2.py::test_comparewithCC_file2 \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 66%]\u001b[0m\n",
      "test_fixtures.py::test_comparewithAA \u001b[31mFAILED\u001b[0m\u001b[31m                              [ 77%]\u001b[0m\n",
      "test_fixtures.py::test_comparewithBB \u001b[32mPASSED\u001b[0m\u001b[31m                              [ 88%]\u001b[0m\n",
      "test_fixtures.py::test_comparewithCC \u001b[31mFAILED\u001b[0m\u001b[31m                              [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ test_comparewithAA ______________________________\u001b[0m\n",
      "\n",
      "supply_AA_BB_CC = [25, 35, 45]\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_comparewithAA\u001b[39;49;00m(supply_AA_BB_CC):\n",
      "        zz=\u001b[94m35\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m supply_AA_BB_CC[\u001b[94m0\u001b[39;49;00m]==zz,\u001b[33m\"\u001b[39;49;00m\u001b[33maa and zz comparison failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: aa and zz comparison failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 25 == 35\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +25\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -35\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_basic_fixture.py\u001b[0m:5: AssertionError\n",
      "\u001b[31m\u001b[1m______________________________ test_comparewithCC ______________________________\u001b[0m\n",
      "\n",
      "supply_AA_BB_CC = [25, 35, 45]\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_comparewithCC\u001b[39;49;00m(supply_AA_BB_CC):\n",
      "        zz=\u001b[94m35\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m supply_AA_BB_CC[\u001b[94m2\u001b[39;49;00m]==zz,\u001b[33m\"\u001b[39;49;00m\u001b[33mcc and zz comparison failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: cc and zz comparison failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 45 == 35\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +45\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -35\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_basic_fixture.py\u001b[0m:13: AssertionError\n",
      "\u001b[31m\u001b[1m___________________________ test_comparewithBB_file2 ___________________________\u001b[0m\n",
      "\n",
      "supply_AA_BB_CC = [25, 35, 45]\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_comparewithBB_file2\u001b[39;49;00m(supply_AA_BB_CC):\n",
      "    \tzz=\u001b[94m25\u001b[39;49;00m\n",
      ">   \t\u001b[94massert\u001b[39;49;00m supply_AA_BB_CC[\u001b[94m1\u001b[39;49;00m]==zz,\u001b[33m\"\u001b[39;49;00m\u001b[33mbb and zz comparison failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE    AssertionError: bb and zz comparison failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE    assert 35 == 25\u001b[0m\n",
      "\u001b[1m\u001b[31mE      +35\u001b[0m\n",
      "\u001b[1m\u001b[31mE      -25\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_basic_fixture2.py\u001b[0m:9: AssertionError\n",
      "\u001b[31m\u001b[1m___________________________ test_comparewithCC_file2 ___________________________\u001b[0m\n",
      "\n",
      "supply_AA_BB_CC = [25, 35, 45]\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_comparewithCC_file2\u001b[39;49;00m(supply_AA_BB_CC):\n",
      "    \tzz=\u001b[94m25\u001b[39;49;00m\n",
      ">   \t\u001b[94massert\u001b[39;49;00m supply_AA_BB_CC[\u001b[94m2\u001b[39;49;00m]==zz,\u001b[33m\"\u001b[39;49;00m\u001b[33mcc and zz comparison failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE    AssertionError: cc and zz comparison failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE    assert 45 == 25\u001b[0m\n",
      "\u001b[1m\u001b[31mE      +45\u001b[0m\n",
      "\u001b[1m\u001b[31mE      -25\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_basic_fixture2.py\u001b[0m:13: AssertionError\n",
      "\u001b[31m\u001b[1m______________________________ test_comparewithAA ______________________________\u001b[0m\n",
      "\n",
      "supply_AA_BB_CC = [25, 35, 45]\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_comparewithAA\u001b[39;49;00m(supply_AA_BB_CC):\n",
      "        zz=\u001b[94m35\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m supply_AA_BB_CC[\u001b[94m0\u001b[39;49;00m]==zz,\u001b[33m\"\u001b[39;49;00m\u001b[33maa and zz comparison failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: aa and zz comparison failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 25 == 35\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +25\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -35\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_fixtures.py\u001b[0m:12: AssertionError\n",
      "\u001b[31m\u001b[1m______________________________ test_comparewithCC ______________________________\u001b[0m\n",
      "\n",
      "supply_AA_BB_CC = [25, 35, 45]\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_comparewithCC\u001b[39;49;00m(supply_AA_BB_CC):\n",
      "        zz=\u001b[94m35\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m supply_AA_BB_CC[\u001b[94m2\u001b[39;49;00m]==zz,\u001b[33m\"\u001b[39;49;00m\u001b[33mcc and zz comparison failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: cc and zz comparison failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 45 == 35\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +45\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -35\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_fixtures.py\u001b[0m:20: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_basic_fixture.py::test_comparewithAA - AssertionError: aa and zz ...\n",
      "FAILED test_basic_fixture.py::test_comparewithCC - AssertionError: cc and zz ...\n",
      "FAILED test_basic_fixture2.py::test_comparewithBB_file2 - AssertionError: bb ...\n",
      "FAILED test_basic_fixture2.py::test_comparewithCC_file2 - AssertionError: cc ...\n",
      "FAILED test_fixtures.py::test_comparewithAA - AssertionError: aa and zz compa...\n",
      "FAILED test_fixtures.py::test_comparewithCC - AssertionError: cc and zz compa...\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m6 failed\u001b[0m, \u001b[32m3 passed\u001b[0m\u001b[31m in 0.06s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test -k test_comparewith -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterized Tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of parameterizing a test is to run a test against multiple sets of arguments. We can do this by @pytest.mark.parametrize.\n",
    "\n",
    "We will see this with the below PyTest example. Here we will pass 3 arguments to a test method. This test method will add the first 2 arguments and compare it with the 3rd argument.\n",
    "\n",
    "Create the test file test_addition.py with the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_addition.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a test_addition.py\n",
    "\n",
    "import pytest\n",
    "@pytest.mark.parametrize(\"input1, input2, output\",[(5,5,10),(3,5,12)])\n",
    "def test_add(input1, input2, output):\n",
    "    assert input1+input2 == output,\"failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the test method accepts 3 arguments- input1, input2, output. It adds input1 and input2 and compares against the output.\n",
    "\n",
    "Let’s run the test by py.test -k test_add -v and see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.7, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /Users/mcoughlin/opt/anaconda3/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/mcoughlin/Code/Teaching/AST8581/ast8581_2022_Spring_workspace/lecture/25\n",
      "plugins: anyio-2.2.0, ligo.skymap-0.6.1\n",
      "collected 11 items / 9 deselected / 2 selected                                 \u001b[0m\n",
      "\n",
      "test_addition.py::test_add[5-5-10] \u001b[32mPASSED\u001b[0m\u001b[32m                                [ 50%]\u001b[0m\n",
      "test_addition.py::test_add[3-5-12] \u001b[31mFAILED\u001b[0m\u001b[31m                                [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_add[3-5-12] _______________________________\u001b[0m\n",
      "\n",
      "input1 = 3, input2 = 5, output = 12\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput1, input2, output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,[(\u001b[94m5\u001b[39;49;00m,\u001b[94m5\u001b[39;49;00m,\u001b[94m10\u001b[39;49;00m),(\u001b[94m3\u001b[39;49;00m,\u001b[94m5\u001b[39;49;00m,\u001b[94m12\u001b[39;49;00m)])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_add\u001b[39;49;00m(input1, input2, output):\n",
      ">       \u001b[94massert\u001b[39;49;00m input1+input2 == output,\u001b[33m\"\u001b[39;49;00m\u001b[33mfailed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 8 == 12\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +8\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -12\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_addition.py\u001b[0m:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_addition.py::test_add[3-5-12] - AssertionError: failed\n",
      "\u001b[31m================== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[31m in 0.05s\u001b[0m\u001b[31m ===================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test -k test_add -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the tests ran 2 times – one checking 5+5 ==10 and other checking 3+5 ==12\n",
    "\n",
    "test_addition.py::test_add[5-5-10] PASSED\n",
    "\n",
    "test_addition.py::test_add[3-5-12] FAILED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xfail / Skip Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be some situations where we don’t want to execute a test, or a test case is not relevant for a particular time. In those situations, we have the option to Xfail the test or skip the tests\n",
    "\n",
    "The xfailed test will be executed, but it will not be counted as part failed or passed tests. There will be no traceback displayed if that test fails. We can xfail tests using\n",
    "\n",
    "@pytest.mark.xfail.\n",
    "\n",
    "Skipping a test means that the test will not be executed. We can skip tests using\n",
    "\n",
    "@pytest.mark.skip.\n",
    "\n",
    "Try the test_addition_2.py with the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_addition_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a test_addition_2.py\n",
    "\n",
    "import pytest\n",
    "@pytest.mark.skip\n",
    "def test_add_1():\n",
    "    assert 100+200 == 400,\"failed\"\n",
    "\n",
    "@pytest.mark.skip\n",
    "def test_add_2():\n",
    "    assert 100+200 == 300,\"failed\"\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_add_3():\n",
    "    assert 15+13 == 28,\"failed\"\n",
    "\n",
    "@pytest.mark.xfail\n",
    "def test_add_4():\n",
    "    assert 15+13 == 100,\"failed\"\n",
    "\n",
    "def test_add_5():\n",
    "    assert 3+2 == 5,\"failed\"\n",
    "\n",
    "def test_add_6():\n",
    "    assert 3+2 == 6,\"failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here\n",
    "\n",
    "* test_add_1 and test_add_2 are skipped and will not be executed.\n",
    "* test_add_3 and test_add_4 are xfailed. These tests will be executed and will be part of xfailed(on test failure) or xpassed(on test pass) tests. There won’t be any traceback for failures.\n",
    "* test_add_5 and test_add_6 will be executed and test_add_6 will report failure with traceback while the test_add_5 passes\n",
    "\n",
    "Execute the test by py.test test_addition_2.py -v and see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.7, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 -- /Users/mcoughlin/opt/anaconda3/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/mcoughlin/Code/Teaching/AST8581/ast8581_2022_Spring_workspace/lecture/25\n",
      "plugins: anyio-2.2.0, ligo.skymap-0.6.1\n",
      "collected 6 items                                                              \u001b[0m\n",
      "\n",
      "test_addition_2.py::test_add_1 \u001b[33mSKIPPED\u001b[0m (unconditional skip)\u001b[32m              [ 16%]\u001b[0m\n",
      "test_addition_2.py::test_add_2 \u001b[33mSKIPPED\u001b[0m (unconditional skip)\u001b[32m              [ 33%]\u001b[0m\n",
      "test_addition_2.py::test_add_3 \u001b[33mXPASS\u001b[0m\u001b[33m                                     [ 50%]\u001b[0m\n",
      "test_addition_2.py::test_add_4 \u001b[33mXFAIL\u001b[0m\u001b[33m                                     [ 66%]\u001b[0m\n",
      "test_addition_2.py::test_add_5 \u001b[32mPASSED\u001b[0m\u001b[33m                                    [ 83%]\u001b[0m\n",
      "test_addition_2.py::test_add_6 \u001b[31mFAILED\u001b[0m\u001b[31m                                    [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________________ test_add_6 __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_add_6\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94m3\u001b[39;49;00m+\u001b[94m2\u001b[39;49;00m == \u001b[94m6\u001b[39;49;00m,\u001b[33m\"\u001b[39;49;00m\u001b[33mfailed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: failed\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert 5 == 6\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +5\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -6\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_addition_2.py\u001b[0m:23: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_addition_2.py::test_add_6 - AssertionError: failed\n",
      "\u001b[31m========= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m, \u001b[33m2 skipped\u001b[0m, \u001b[33m1 xfailed\u001b[0m, \u001b[33m1 xpassed\u001b[0m\u001b[31m in 0.07s\u001b[0m\u001b[31m =========\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test test_addition_2.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit testing and continuous integration\n",
    "\n",
    "A good habit to get into is running the full testing suite of our project at every commit. In fact, it is even possible to do this completely transparently and automatically through continuous integration. We can set up a server that automatically runs our testing suite in the cloud at every commit. If a test fails, we get an automatic e-mail telling us what the problem is so that we can fix it.\n",
    "\n",
    "There are many continuous integration systems and services: Jenkins/Hudson, Travis CI (https://travis-ci.org), Codeship (http://codeship.com/), and others. Some of them play well with GitHub. For example, to use Github Actions with a GitHub project, one just needs to add appropriately formatted yml files in .github/workflows with various settings in your repository (see https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, unit testing, code coverage, and continuous integration are standard practices that should be used in all significant projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class exercise: Write one or more unit tests for your group project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "lecture24.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
