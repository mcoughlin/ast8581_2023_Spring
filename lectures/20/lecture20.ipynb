{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': 1920, 'height': 1080, 'scroll': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1920,\n",
    "        'height': 1080,\n",
    "        'scroll': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 11 (Wednesday), AST 8581 / PHYS 8581 / CSCI 8581 / STAT 8581: Big Data in Astrophysics\n",
    "\n",
    "### Michael Coughlin <cough052@umn.edu>, Jie Ding <dingj@umn.edu>\n",
    "\n",
    "With contributions totally ripped off from Dima Duev (Weights and Biases), Lex Friedman (MIT), François Chollet (Google), Michael Steinbach (UMN), and Nico Adams (UMN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where do we stand?\n",
    "\n",
    "Foundations of Data and Probability -> Statistical frameworks (Frequentist vs Bayesian) -> Estimating underlying distributions -> Analysis of Time series (periodicity) -> Analysis of Time series (variability) -> Analysis of Time series (stochastic processes) -> Gaussian Processes -> Decision Trees / Regression -> Dimensionality Reduction -> Principle Component Analysis -> Clustering -> Density Estimation / Anomaly Detection -> Supervised Learning -> Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/dl11.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/dl12.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More: Recipe / Practical advice from one of DL godfathers Andrej Karpathy\n",
    "\n",
    "Highly recommended: [Andrej Karpathy's blog post](http://karpathy.github.io/2019/04/25/recipe/)\n",
    "\n",
    "- Neural net training is a leaky abstraction\n",
    "\n",
    "```bash\n",
    ">>> your_data = # plug your awesome dataset here\n",
    ">>> model = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)\n",
    "# conquer world here\n",
    "```\n",
    "\n",
    "- Neural net training fails silently\n",
    "\n",
    "Lots of ways to screw things up -> many paths to pain and suffering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "To find a good model takes two stages: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss).\n",
    "\n",
    "### Tips & tricks for this stage:\n",
    "\n",
    "- **picking the model**. To reach a good training loss you’ll want to choose an appropriate architecture for the data. When it comes to choosing this my #1 advice is: Don’t be a hero. I’ve seen a lot of people who are eager to get crazy and creative in stacking up the lego blocks of the neural net toolbox in various exotic architectures that make sense to them. Resist this temptation strongly in the early stages of your project. I always advise people to simply find the most related paper and copy paste their simplest architecture that achieves good performance. E.g. if you are classifying images don’t be a hero and just copy paste a ResNet-50 for your first run. You’re allowed to do something more custom later and beat this.\n",
    "- **adam is safe**. In the early stages of setting baselines I like to use Adam with a learning rate of 3e-4. In my experience Adam is much more forgiving to hyperparameters, including a bad learning rate. For ConvNets a well-tuned SGD will almost always slightly outperform Adam, but the optimal learning rate region is much more narrow and problem-specific. (Note: If you are using RNNs and related sequence models it is more common to use Adam. At the initial stage of your project, again, don’t be a hero and follow whatever the most related papers do.)\n",
    "- **complexify only one at a time**. If you have multiple signals to plug into your classifier I would advise that you plug them in one by one and every time ensure that you get a performance boost you’d expect. Don’t throw the kitchen sink at your model at the start. There are other ways of building up complexity - e.g. you can try to plug in smaller images first and make them bigger later, etc.\n",
    "- **do not trust learning rate decay defaults**. If you are re-purposing code from some other domain always be very careful with learning rate decay. Not only would you want to use different decay schedules for different problems, but - even worse - in a typical implementation the schedule will be based current epoch number, which can vary widely simply depending on the size of your dataset. E.g. ImageNet would decay by 10 on epoch 30. If you’re not training ImageNet then you almost certainly do not want this. If you’re not careful your code could secretely be driving your learning rate to zero too early, not allowing your model to converge. In my own work I always disable learning rate decays entirely (I use a constant LR) and tune this all the way at the very end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularize\n",
    "\n",
    "Now to regularize the model and gain some validation accuracy by giving up some of the training accuracy.\n",
    "\n",
    "### Tips & tricks for this stage:\n",
    "\n",
    "- **get more data. First**, the by far best and preferred way to regularize a model in any practical setting is to add more real training data. It is a very common mistake to spend a lot engineering cycles trying to squeeze juice out of a small dataset when you could instead be collecting more data. As far as I’m aware adding more data is pretty much the only guaranteed way to monotonically improve the performance of a well-configured neural network almost indefinitely. The other would be ensembles (if you can afford them), but that tops out after ~5 models.\n",
    "- **data augment**. The next best thing to real data is half-fake data - try out more aggressive data augmentation.\n",
    "- **creative augmentation**. If half-fake data doesn’t do it, fake data may also do something. People are finding creative ways of expanding datasets; For example, domain randomization, use of simulation, clever hybrids such as inserting (potentially simulated) data into scenes, or even GANs.\n",
    "- **pretrain**. It rarely ever hurts to use a pretrained network if you can, even if you have enough data.\n",
    "- **stick with supervised learning**. Do not get over-excited about unsupervised pretraining. Unlike what that blog post from 2008 tells you, as far as I know, no version of it has reported strong results in modern computer vision (though NLP seems to be doing pretty well with BERT and friends these days, quite likely owing to the more deliberate nature of text, and a higher signal to noise ratio).\n",
    "- **smaller input dimensionality**. Remove features that may contain spurious signal. Any added spurious input is just another opportunity to overfit if your dataset is small. Similarly, if low-level details don’t matter much try to input a smaller image.\n",
    "- **smaller model size**. In many cases you can use domain knowledge constraints on the network to decrease its size. As an example, it used to be trendy to use Fully Connected layers at the top of backbones for ImageNet but these have since been replaced with simple average pooling, eliminating a ton of parameters in the process.\n",
    "- **decrease the batch size**. Due to the normalization inside batch norm smaller batch sizes somewhat correspond to stronger regularization. This is because the batch empirical mean/std are more approximate versions of the full mean/std so the scale & offset “wiggles” your batch around more.\n",
    "- **drop**. Add dropout. Use dropout2d (spatial dropout) for ConvNets. Use this sparingly/carefully because dropout does not seem to play nice with batch normalization.\n",
    "- **weight decay**. Increase the weight decay penalty.\n",
    "- **early stopping**. Stop training based on your measured validation loss to catch your model just as it’s about to overfit.\n",
    "- **try a larger model**. I mention this last and only after early stopping but I’ve found a few times in the past that larger models will of course overfit much more eventually, but their “early stopped” performance can often be much better than that of smaller models.\n",
    "\n",
    "Finally, to gain additional confidence that your network is a reasonable classifier, I like to visualize the network’s first-layer weights and ensure you get nice edges that make sense. If your first layer filters look like noise then something could be off. Similarly, activations inside the net can sometimes display odd artifacts and hint at problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune\n",
    "\n",
    "You should now be “in the loop” with your dataset exploring a wide model space for architectures that achieve low validation loss. A few tips and tricks for this step:\n",
    "\n",
    "### Tips & tricks for this stage:\n",
    "\n",
    "- ** random over grid search**. For simultaneously tuning multiple hyperparameters it may sound tempting to use grid search to ensure coverage of all settings, but keep in mind that it is best to use random search instead. Intuitively, this is because neural nets are often much more sensitive to some parameters than others. In the limit, if a parameter a matters but changing b has no effect then you’d rather sample a more throughly than at a few fixed points multiple times.\n",
    "- **hyper-parameter optimization**. There is a large number of fancy bayesian hyper-parameter optimization toolboxes around and a few of my friends have also reported success with them, but my personal experience is that the state of the art approach to exploring a nice and wide space of models and hyperparameters is to use an intern :). Just kidding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze out the juice\n",
    "\n",
    "Once you find the best types of architectures and hyper-parameters you can still use a few more tricks to squeeze out the last pieces of juice out of the system:\n",
    "\n",
    "### Tips & tricks for this stage:\n",
    "\n",
    "- **ensembles**. Model ensembles are a pretty much guaranteed way to gain 2% of accuracy on anything. If you can’t afford the computation at test time look into distilling your ensemble into a network using dark knowledge.\n",
    "- **leave it training**. I’ve often seen people tempted to stop the model training when the validation loss seems to be leveling off. In my experience networks keep training for unintuitively long time. One time I accidentally left a model training during the winter break and when I got back in January it was SOTA (“state of the art”).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class example: Classifying movie reviews: a binary classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB dataset\n",
    "\n",
    "\n",
    "We'll be working with \"IMDB dataset\", a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000 \n",
    "reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.\n",
    "It has already been preprocessed: the reviews (sequences of words) \n",
    "have been turned into sequences of integers, where each integer stands for a specific word in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and underfitting\n",
    "\n",
    "Overfitting happens in every single machine learning \n",
    "problem. Learning how to deal with overfitting is essential to mastering machine learning.\n",
    "\n",
    "The fundamental issue in machine learning is the tension between optimization and generalization. \"Optimization\" refers to the process of \n",
    "adjusting a model to get the best performance possible on the training data (the \"learning\" in \"machine learning\"), while \"generalization\" \n",
    "refers to how well the trained model would perform on data it has never seen before. The goal of the game is to get good generalization, of \n",
    "course, but you do not control generalization; you can only adjust the model based on its training data.\n",
    "\n",
    "At the beginning of training, optimization and generalization are correlated: the lower your loss on training data, the lower your loss on \n",
    "test data. While this is happening, your model is said to be _under-fit_: there is still progress to be made; the network hasn't yet \n",
    "modeled all relevant patterns in the training data. But after a certain number of iterations on the training data, generalization stops \n",
    "improving, validation metrics stall then start degrading: the model is then starting to over-fit, i.e. is it starting to learn patterns \n",
    "that are specific to the training data but that are misleading or irrelevant when it comes to new data.\n",
    "\n",
    "To prevent a model from learning misleading or irrelevant patterns found in the training data, _the best solution is of course to get \n",
    "more training data_. A model trained on more data will naturally generalize better. When that is no longer possible, the next best solution \n",
    "is to modulate the quantity of information that your model is allowed to store, or to add constraints on what information it is allowed to \n",
    "store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most \n",
    "prominent patterns, which have a better chance of generalizing well.\n",
    "\n",
    "The processing of fighting overfitting in this way is called _regularization_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)\n",
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fighting overfitting: Reducing the network's size\n",
    "\n",
    "The simplest way to prevent overfitting is to reduce the size of the model, i.e. the number of learnable parameters in the model (which is \n",
    "determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is \n",
    "often referred to as the model's \"capacity\". Intuitively, a model with more parameters will have more \"memorization capacity\" and therefore \n",
    "will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any \n",
    "generalization power. For instance, a model with 500,000 binary parameters could easily be made to learn the class of every digits in the \n",
    "MNIST training set: we would only need 10 binary parameters for each of the 50,000 digits. Such a model would be useless for classifying \n",
    "new digit samples. Always keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge \n",
    "is generalization, not fitting.\n",
    "\n",
    "On the other hand, if the network has limited memorization resources, it will not be able to learn this mapping as easily, and thus, in \n",
    "order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets \n",
    "-- precisely the type of representations that we are interested in. At the same time, keep in mind that you should be using models that have \n",
    "enough parameters that they won't be underfitting: your model shouldn't be starved for memorization resources. There is a compromise to be \n",
    "found between \"too much capacity\" and \"not enough capacity\".\n",
    "\n",
    "Unfortunately, there is no magical formula to determine what the right number of layers is, or what the right size for each layer is. You \n",
    "will have to evaluate an array of different architectures (on your validation set, not on your test set, of course) in order to find the \n",
    "right model size for your data. The general workflow to find an appropriate model size is to start with relatively few layers and \n",
    "parameters, and start increasing the size of the layers or adding new layers until you see diminishing returns with regard to the \n",
    "validation loss.\n",
    "\n",
    "Let's try this on our movie review classification network. Our original network was as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "original_model = models.Sequential()\n",
    "original_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "original_model.add(layers.Dense(16, activation='relu'))\n",
    "original_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "original_model.compile(optimizer='rmsprop',\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to replace it with this smaller network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_model = models.Sequential()\n",
    "smaller_model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "smaller_model.add(layers.Dense(4, activation='relu'))\n",
    "smaller_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "smaller_model.compile(optimizer='rmsprop',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's a comparison of the validation losses of the original network and the smaller network. The dots are the validation loss values of \n",
    "the smaller network, and the crosses are the initial network (remember: a lower validation loss signals a better model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 6s 256us/step - loss: 0.4373 - acc: 0.8282 - val_loss: 0.3293 - val_acc: 0.8818\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 3s 114us/step - loss: 0.2542 - acc: 0.9094 - val_loss: 0.2917 - val_acc: 0.8849\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 4s 142us/step - loss: 0.1972 - acc: 0.9290 - val_loss: 0.2923 - val_acc: 0.8822\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 3s 121us/step - loss: 0.1627 - acc: 0.9429 - val_loss: 0.3091 - val_acc: 0.8774\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 3s 112us/step - loss: 0.1414 - acc: 0.9504 - val_loss: 0.3808 - val_acc: 0.8560\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 3s 111us/step - loss: 0.1234 - acc: 0.9579 - val_loss: 0.3411 - val_acc: 0.8757\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 3s 115us/step - loss: 0.1090 - acc: 0.9638 - val_loss: 0.3643 - val_acc: 0.8690\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 3s 114us/step - loss: 0.0980 - acc: 0.9676 - val_loss: 0.3884 - val_acc: 0.8668\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 3s 128us/step - loss: 0.0861 - acc: 0.9718 - val_loss: 0.4088 - val_acc: 0.8661\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 3s 127us/step - loss: 0.0730 - acc: 0.9766 - val_loss: 0.4415 - val_acc: 0.8627\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 4s 158us/step - loss: 0.0654 - acc: 0.9787 - val_loss: 0.4705 - val_acc: 0.8630\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 3s 124us/step - loss: 0.0588 - acc: 0.9824 - val_loss: 0.5448 - val_acc: 0.8592\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 3s 122us/step - loss: 0.0514 - acc: 0.9848 - val_loss: 0.5350 - val_acc: 0.8599\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 3s 120us/step - loss: 0.0433 - acc: 0.9876 - val_loss: 0.5640 - val_acc: 0.8594\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 3s 120us/step - loss: 0.0389 - acc: 0.9894 - val_loss: 0.5915 - val_acc: 0.8561\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 3s 118us/step - loss: 0.0323 - acc: 0.9910 - val_loss: 0.6245 - val_acc: 0.8553\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 3s 126us/step - loss: 0.0268 - acc: 0.9936 - val_loss: 0.6913 - val_acc: 0.8543\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 3s 121us/step - loss: 0.0258 - acc: 0.9932 - val_loss: 0.6955 - val_acc: 0.8550\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 3s 127us/step - loss: 0.0212 - acc: 0.9945 - val_loss: 0.7669 - val_acc: 0.8406\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 3s 131us/step - loss: 0.0180 - acc: 0.9952 - val_loss: 0.7770 - val_acc: 0.8535\n"
     ]
    }
   ],
   "source": [
    "original_hist = original_model.fit(x_train, y_train,\n",
    "                                   epochs=20,\n",
    "                                   batch_size=512,\n",
    "                                   validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 4s 169us/step - loss: 0.6110 - acc: 0.6874 - val_loss: 0.5552 - val_acc: 0.7641\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 6s 222us/step - loss: 0.4951 - acc: 0.8489 - val_loss: 0.4724 - val_acc: 0.8557\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 10s 380us/step - loss: 0.3889 - acc: 0.9018 - val_loss: 0.3836 - val_acc: 0.8750\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 10s 402us/step - loss: 0.2929 - acc: 0.9231 - val_loss: 0.3225 - val_acc: 0.8879\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 10s 396us/step - loss: 0.2347 - acc: 0.9322 - val_loss: 0.2999 - val_acc: 0.8868\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 9s 349us/step - loss: 0.2005 - acc: 0.9398 - val_loss: 0.2898 - val_acc: 0.8856\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 7s 291us/step - loss: 0.1771 - acc: 0.9454 - val_loss: 0.2868 - val_acc: 0.8873\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 6s 259us/step - loss: 0.1601 - acc: 0.9494 - val_loss: 0.2996 - val_acc: 0.8813\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 6s 250us/step - loss: 0.1458 - acc: 0.9538 - val_loss: 0.3041 - val_acc: 0.8793\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 6s 253us/step - loss: 0.1342 - acc: 0.9587 - val_loss: 0.3076 - val_acc: 0.8810\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 8s 305us/step - loss: 0.1244 - acc: 0.9611 - val_loss: 0.3271 - val_acc: 0.8747\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 7s 288us/step - loss: 0.1153 - acc: 0.9644 - val_loss: 0.3314 - val_acc: 0.8761\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 5s 209us/step - loss: 0.1073 - acc: 0.9665 - val_loss: 0.3443 - val_acc: 0.8741\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 5s 201us/step - loss: 0.1001 - acc: 0.9702 - val_loss: 0.3591 - val_acc: 0.8720\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 6s 259us/step - loss: 0.0935 - acc: 0.9725 - val_loss: 0.3792 - val_acc: 0.8685\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 6s 222us/step - loss: 0.0871 - acc: 0.9750 - val_loss: 0.3946 - val_acc: 0.8686\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 5s 213us/step - loss: 0.0821 - acc: 0.9770 - val_loss: 0.4082 - val_acc: 0.8664\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 5s 201us/step - loss: 0.0766 - acc: 0.9787 - val_loss: 0.4249 - val_acc: 0.8648\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 5s 204us/step - loss: 0.0719 - acc: 0.9797 - val_loss: 0.4457 - val_acc: 0.8642\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 5s 214us/step - loss: 0.0672 - acc: 0.9825 - val_loss: 0.4590 - val_acc: 0.8616\n"
     ]
    }
   ],
   "source": [
    "smaller_model_hist = smaller_model.fit(x_train, y_train,\n",
    "                                       epochs=20,\n",
    "                                       batch_size=512,\n",
    "                                       validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 21)\n",
    "original_val_loss = original_hist.history['val_loss']\n",
    "smaller_model_val_loss = smaller_model_hist.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# b+ is for \"blue cross\"\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, smaller_model_val_loss, 'bo', label='Smaller model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, the smaller network starts overfitting later than the reference one (after 6 epochs rather than 4) and its performance \n",
    "degrades much more slowly once it starts overfitting.\n",
    "\n",
    "Now, for kicks, let's add to this benchmark a network that has much more capacity, far more than the problem would warrant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_model = models.Sequential()\n",
    "bigger_model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\n",
    "bigger_model.add(layers.Dense(512, activation='relu'))\n",
    "bigger_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "bigger_model.compile(optimizer='rmsprop',\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 18s 738us/step - loss: 0.4465 - acc: 0.8006 - val_loss: 0.2816 - val_acc: 0.8889\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 16s 634us/step - loss: 0.2086 - acc: 0.9188 - val_loss: 0.3050 - val_acc: 0.8810\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 15s 592us/step - loss: 0.1185 - acc: 0.9602 - val_loss: 0.5612 - val_acc: 0.8266\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 17s 666us/step - loss: 0.0645 - acc: 0.9813 - val_loss: 0.4652 - val_acc: 0.8819\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 22s 875us/step - loss: 0.0864 - acc: 0.9884 - val_loss: 0.5099 - val_acc: 0.8789\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 22s 883us/step - loss: 0.0059 - acc: 0.9985 - val_loss: 5.4485 - val_acc: 0.5818\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 23s 905us/step - loss: 0.0902 - acc: 0.9906 - val_loss: 0.7073 - val_acc: 0.8791\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 21s 828us/step - loss: 0.1086 - acc: 0.9879 - val_loss: 0.6128 - val_acc: 0.8606\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 20s 804us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.6996 - val_acc: 0.8780\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 26s 1ms/step - loss: 0.0607 - acc: 0.9911 - val_loss: 0.6387 - val_acc: 0.8774\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 22s 875us/step - loss: 2.5968e-04 - acc: 1.0000 - val_loss: 0.8177 - val_acc: 0.8774\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 21s 822us/step - loss: 3.4178e-05 - acc: 1.0000 - val_loss: 1.0233 - val_acc: 0.8786\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 22s 884us/step - loss: 0.1175 - acc: 0.9897 - val_loss: 0.7631 - val_acc: 0.8801\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 18s 728us/step - loss: 4.1876e-05 - acc: 1.0000 - val_loss: 0.8460 - val_acc: 0.8797\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 17s 674us/step - loss: 1.3157e-05 - acc: 1.0000 - val_loss: 0.9480 - val_acc: 0.8796\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 20s 818us/step - loss: 3.7221e-06 - acc: 1.0000 - val_loss: 1.0655 - val_acc: 0.8797\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 18s 736us/step - loss: 7.8734e-07 - acc: 1.0000 - val_loss: 1.2153 - val_acc: 0.8796\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 17s 674us/step - loss: 0.0911 - acc: 0.9940 - val_loss: 3.3051 - val_acc: 0.7500\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 18s 723us/step - loss: 0.0228 - acc: 0.9978 - val_loss: 0.9203 - val_acc: 0.8793\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 25s 1ms/step - loss: 3.1404e-06 - acc: 1.0000 - val_loss: 0.9664 - val_acc: 0.8795\n"
     ]
    }
   ],
   "source": [
    "bigger_model_hist = bigger_model.fit(x_train, y_train,\n",
    "                                     epochs=20,\n",
    "                                     batch_size=512,\n",
    "                                     validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the bigger network fares compared to the reference one. The dots are the validation loss values of the bigger network, and the \n",
    "crosses are the initial network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigger_model_val_loss = bigger_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, bigger_model_val_loss, 'bo', label='Bigger model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The bigger network starts overfitting almost right away, after just one epoch, and overfits much more severely. Its validation loss is also \n",
    "more noisy.\n",
    "\n",
    "Meanwhile, here are the training losses for our two networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_train_loss = original_hist.history['loss']\n",
    "bigger_model_train_loss = bigger_model_hist.history['loss']\n",
    "\n",
    "plt.plot(epochs, original_train_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, bigger_model_train_loss, 'bo', label='Bigger model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the bigger network gets its training loss near zero very quickly. The more capacity the network has, the quicker it will be \n",
    "able to model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large \n",
    "difference between the training and validation loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding weight regularization\n",
    "\n",
    "\n",
    "You may be familiar with _Occam's Razor_ principle: given two explanations for something, the explanation most likely to be correct is the \n",
    "\"simplest\" one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some \n",
    "training data and a network architecture, there are multiple sets of weights values (multiple _models_) that could explain the data, and \n",
    "simpler models are less likely to overfit than complex ones.\n",
    "\n",
    "A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer \n",
    "parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity \n",
    "of a network by forcing its weights to only take small values, which makes the distribution of weight values more \"regular\". This is called \n",
    "\"weight regularization\", and it is done by adding to the loss function of the network a _cost_ associated with having large weights. This \n",
    "cost comes in two flavors:\n",
    "\n",
    "* L1 regularization, where the cost added is proportional to the _absolute value of the weights coefficients_ (i.e. to what is called the \n",
    "\"L1 norm\" of the weights).\n",
    "* L2 regularization, where the cost added is proportional to the _square of the value of the weights coefficients_ (i.e. to what is called \n",
    "the \"L2 norm\" of the weights). L2 regularization is also called _weight decay_ in the context of neural networks. Don't let the different \n",
    "name confuse you: weight decay is mathematically the exact same as L2 regularization.\n",
    "\n",
    "In Keras, weight regularization is added by passing _weight regularizer instances_ to layers as keyword arguments. Let's add L2 weight \n",
    "regularization to our movie review classification network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "l2_model = models.Sequential()\n",
    "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                          activation='relu', input_shape=(10000,)))\n",
    "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                          activation='relu'))\n",
    "l2_model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model.compile(optimizer='rmsprop',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`l2(0.001)` means that every coefficient in the weight matrix of the layer will add `0.001 * weight_coefficient_value` to the total loss of \n",
    "the network. Note that because this penalty is _only added at training time_, the loss for this network will be much higher at training \n",
    "than at test time.\n",
    "\n",
    "Here's the impact of our L2 regularization penalty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 3s 133us/step - loss: 0.4922 - acc: 0.8204 - val_loss: 0.4052 - val_acc: 0.8552\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 3s 132us/step - loss: 0.3114 - acc: 0.9070 - val_loss: 0.3622 - val_acc: 0.8710\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.2704 - acc: 0.9198 - val_loss: 0.3340 - val_acc: 0.8861\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 4s 175us/step - loss: 0.2483 - acc: 0.9277 - val_loss: 0.3313 - val_acc: 0.8880\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 5s 180us/step - loss: 0.2354 - acc: 0.9351 - val_loss: 0.3398 - val_acc: 0.8836\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 5s 182us/step - loss: 0.2299 - acc: 0.9360 - val_loss: 0.3690 - val_acc: 0.8749\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 5s 202us/step - loss: 0.2225 - acc: 0.9385 - val_loss: 0.3525 - val_acc: 0.8805\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 5s 180us/step - loss: 0.2138 - acc: 0.9436 - val_loss: 0.3676 - val_acc: 0.8749\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 5s 183us/step - loss: 0.2129 - acc: 0.9421 - val_loss: 0.3686 - val_acc: 0.8764\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 4s 171us/step - loss: 0.2065 - acc: 0.9444 - val_loss: 0.3718 - val_acc: 0.8746\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 4s 164us/step - loss: 0.2047 - acc: 0.9463 - val_loss: 0.3801 - val_acc: 0.8747\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 4s 161us/step - loss: 0.2038 - acc: 0.9462 - val_loss: 0.3832 - val_acc: 0.8737\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 5s 193us/step - loss: 0.1960 - acc: 0.9506 - val_loss: 0.4663 - val_acc: 0.8537\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 5s 199us/step - loss: 0.1953 - acc: 0.9490 - val_loss: 0.4529 - val_acc: 0.8572\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 5s 200us/step - loss: 0.1922 - acc: 0.9509 - val_loss: 0.4193 - val_acc: 0.8657\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 6s 236us/step - loss: 0.1907 - acc: 0.9516 - val_loss: 0.4193 - val_acc: 0.8667\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 6s 243us/step - loss: 0.1850 - acc: 0.9540 - val_loss: 0.4558 - val_acc: 0.8608\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 7s 285us/step - loss: 0.1820 - acc: 0.9557 - val_loss: 0.4092 - val_acc: 0.8707\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 5s 199us/step - loss: 0.1802 - acc: 0.9559 - val_loss: 0.4667 - val_acc: 0.8545\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 4s 178us/step - loss: 0.1787 - acc: 0.9576 - val_loss: 0.4255 - val_acc: 0.8682\n"
     ]
    }
   ],
   "source": [
    "l2_model_hist = l2_model.fit(x_train, y_train,\n",
    "                             epochs=20,\n",
    "                             batch_size=512,\n",
    "                             validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l2_model_val_loss = l2_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, l2_model_val_loss, 'bo', label='L2-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As you can see, the model with L2 regularization (dots) has become much more resistant to overfitting than the reference model (crosses), \n",
    "even though both models have the same number of parameters.\n",
    "\n",
    "As alternatives to L2 regularization, you could use one of the following Keras weight regularizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.regularizers.L1L2 at 0x7f8cbbd79210>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "# L1 regularization\n",
    "regularizers.l1(0.001)\n",
    "\n",
    "# L1 and L2 regularization at the same time\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding dropout\n",
    "\n",
    "\n",
    "Dropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his \n",
    "students at the University of Toronto. Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. setting to zero) a number of \n",
    "output features of the layer during training. Let's say a given layer would normally have returned a vector `[0.2, 0.5, 1.3, 0.8, 1.1]` for a \n",
    "given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. `[0, 0.5, \n",
    "1.3, 0, 1.1]`. The \"dropout rate\" is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test \n",
    "time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to \n",
    "balance for the fact that more units are active than at training time.\n",
    "\n",
    "Consider a Numpy matrix containing the output of a layer, `layer_output`, of shape `(batch_size, features)`. At training time, we would be \n",
    "zero-ing out at random a fraction of the values in the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At training time: we drop out 50% of the units in the output\n",
    "layer_output *= np.randint(0, high=2, size=layer_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "At test time, we would be scaling the output down by the dropout rate. Here we scale by 0.5 (because we were previous dropping half the \n",
    "units):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# At test time:\n",
    "layer_output *= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that this process can be implemented by doing both operations at training time and leaving the output unchanged at test time, which is \n",
    "often the way it is implemented in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# At training time:\n",
    "layer_output *= np.randint(0, high=2, size=layer_output.shape)\n",
    "# Note that we are scaling *up* rather scaling *down* in this case\n",
    "layer_output /= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This technique may seem strange and arbitrary. Why would this help reduce overfitting? Geoff Hinton has said that he was inspired, among \n",
    "other things, by a fraud prevention mechanism used by banks -- in his own words: _\"I went to my bank. The tellers kept changing and I asked \n",
    "one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation \n",
    "between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each \n",
    "example would prevent conspiracies and thus reduce overfitting\"_.\n",
    "\n",
    "The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that are not significant (what \n",
    "Hinton refers to as \"conspiracies\"), which the network would start memorizing if no noise was present. \n",
    "\n",
    "In Keras you can introduce dropout in a network via the `Dropout` layer, which gets applied to the output of layer right before it, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add two `Dropout` layers in our IMDB network to see how well they do at reducing overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpt_model = models.Sequential()\n",
    "dpt_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "dpt_model.add(layers.Dropout(0.5))\n",
    "dpt_model.add(layers.Dense(16, activation='relu'))\n",
    "dpt_model.add(layers.Dropout(0.5))\n",
    "dpt_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "dpt_model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 4s 166us/step - loss: 0.6044 - acc: 0.6854 - val_loss: 0.5050 - val_acc: 0.8606\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 3s 126us/step - loss: 0.4775 - acc: 0.8091 - val_loss: 0.3988 - val_acc: 0.8772\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 3s 137us/step - loss: 0.3982 - acc: 0.8637 - val_loss: 0.3443 - val_acc: 0.8838\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 3s 140us/step - loss: 0.3412 - acc: 0.8886 - val_loss: 0.3146 - val_acc: 0.8827\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 3s 139us/step - loss: 0.2970 - acc: 0.9052 - val_loss: 0.3025 - val_acc: 0.8833\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 3s 139us/step - loss: 0.2648 - acc: 0.9171 - val_loss: 0.3027 - val_acc: 0.8845\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 3s 137us/step - loss: 0.2344 - acc: 0.9258 - val_loss: 0.3082 - val_acc: 0.8805\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 4s 141us/step - loss: 0.2171 - acc: 0.9315 - val_loss: 0.3375 - val_acc: 0.8840\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 4s 147us/step - loss: 0.1976 - acc: 0.9374 - val_loss: 0.3343 - val_acc: 0.8742\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 4s 157us/step - loss: 0.1838 - acc: 0.9422 - val_loss: 0.3610 - val_acc: 0.8800\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 4s 154us/step - loss: 0.1739 - acc: 0.9433 - val_loss: 0.3822 - val_acc: 0.8787\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.1603 - acc: 0.9478 - val_loss: 0.3906 - val_acc: 0.8765\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 4s 165us/step - loss: 0.1563 - acc: 0.9492 - val_loss: 0.4294 - val_acc: 0.8785\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 4s 167us/step - loss: 0.1482 - acc: 0.9504 - val_loss: 0.4354 - val_acc: 0.8751\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 0.1394 - acc: 0.9541 - val_loss: 0.4738 - val_acc: 0.8774\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 4s 162us/step - loss: 0.1378 - acc: 0.9553 - val_loss: 0.4842 - val_acc: 0.8756\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 4s 174us/step - loss: 0.1347 - acc: 0.9549 - val_loss: 0.5189 - val_acc: 0.8762\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 5s 183us/step - loss: 0.1315 - acc: 0.9577 - val_loss: 0.5351 - val_acc: 0.8747\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 5s 191us/step - loss: 0.1289 - acc: 0.9590 - val_loss: 0.5485 - val_acc: 0.8704\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 5s 193us/step - loss: 0.1282 - acc: 0.9573 - val_loss: 0.5600 - val_acc: 0.8702\n"
     ]
    }
   ],
   "source": [
    "dpt_model_hist = dpt_model.fit(x_train, y_train,\n",
    "                               epochs=20,\n",
    "                               batch_size=512,\n",
    "                               validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dpt_model_val_loss = dpt_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, dpt_model_val_loss, 'bo', label='Dropout-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Again, a clear improvement over the reference network.\n",
    "\n",
    "To recap: here the most common ways to prevent overfitting in neural networks:\n",
    "\n",
    "* Getting more training data.\n",
    "* Reducing the capacity of the network.\n",
    "* Adding weight regularization.\n",
    "* Adding dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class Exercise: Real-bogus classification for the Zwicky Transient Facility (ZTF) using deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient automated detection of flux-transient, reoccurring flux-variable, and moving objects is increasingly important for large-scale astronomical surveys.\n",
    "\n",
    "* Events observed by ZTF may have been triggered from a flux-transient, a reoccurring flux-variable, or a moving object. The metadata and contextual information including the cutouts are put into \"alert packets\" that are distributed via the ZTF Alert Distribution System (ZADS). On a typical night, the number of detected events ranges from $10^5 - 10^6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/fig-ztf_alerts.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The real/bogus ($rb$) machine learning (ML) classifiers are designed to separate genuine astrophysical events from bogus detections by scoring individual sources on a scale from 0.0 (bogus) to 1.0 (real). Currently, ZTF employs two $rb$ classifiers: a feature-based random forest classifier ($rfrb$), and `braai` a convolutional-neural-network, deep-learning classifier.\n",
    "* In this tutorial, we will build a deep $rb$ classifier based on `braai`. We will use a data set consisting of $11.5k$ labeled alerts from the [ZTF public alert stream](https://ztf.uw.edu/alerts/public/).\n",
    "* For further details on `braai` please refer to For details, please see [Duev+ 2019, MNRAS, 489, 3582](https://academic.oup.com/mnras/article/489/3/3582/5554758) or [arXiv:1907.11259](https://arxiv.org/pdf/1907.11259.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from astropy.time import Time\n",
    "\n",
    "import os\n",
    "import io\n",
    "import gzip\n",
    "from astropy.io import fits\n",
    "# pip install pymongo\n",
    "from bson.json_util import loads, dumps\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use(['dark_background'])\n",
    "from pandas.plotting import register_matplotlib_converters, scatter_matrix\n",
    "register_matplotlib_converters()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set\n",
    "\n",
    "Download from `skipper` if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O candidates.csv https://storage.googleapis.com/ztf-braai/braai.candidates.programid1.csv\n",
    "!wget -O triplets.norm.npy https://storage.googleapis.com/ztf-braai/braai.triplets.norm.programid1.npy\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candidates\n",
    "\n",
    "First load the $csv$ file containing the `candidate` block of the alerts (cutouts and previous detections are excluded). All alerts are labeled (0=bogus, 1=real)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/candidates.csv')\n",
    "display(df)\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'num_bogus: {np.sum(df.label == 0)}')\n",
    "print(f'num_real: {np.sum(df.label == 1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cutout images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_triplet(alert, normalize: bool = False, to_tpu: bool = False):\n",
    "    \"\"\"\n",
    "        Feed in alert packet\n",
    "    \"\"\"\n",
    "    cutout_dict = dict()\n",
    "\n",
    "    for cutout in ('science', 'template', 'difference'):\n",
    "        cutout_data = loads(dumps([alert[f'cutout{cutout.capitalize()}']['stampData']]))[0]\n",
    "\n",
    "        # unzip\n",
    "        with gzip.open(io.BytesIO(cutout_data), 'rb') as f:\n",
    "            with fits.open(io.BytesIO(f.read())) as hdu:\n",
    "                data = hdu[0].data\n",
    "                # replace nans with zeros\n",
    "                cutout_dict[cutout] = np.nan_to_num(data)\n",
    "                # normalize\n",
    "                if normalize:\n",
    "                    cutout_dict[cutout] /= np.linalg.norm(cutout_dict[cutout])\n",
    "\n",
    "        # pad to 63x63 if smaller\n",
    "        shape = cutout_dict[cutout].shape\n",
    "        if shape != (63, 63):\n",
    "            # print(f'Shape of {candid}/{cutout}: {shape}, padding to (63, 63)')\n",
    "            cutout_dict[cutout] = np.pad(cutout_dict[cutout], [(0, 63 - shape[0]), (0, 63 - shape[1])],\n",
    "                                         mode='constant', constant_values=1e-9)\n",
    "\n",
    "    triplet = np.zeros((63, 63, 3))\n",
    "    triplet[:, :, 0] = cutout_dict['science']\n",
    "    triplet[:, :, 1] = cutout_dict['template']\n",
    "    triplet[:, :, 2] = cutout_dict['difference']\n",
    "    \n",
    "    if to_tpu:\n",
    "        # Edge TPUs require additional processing\n",
    "        triplet = np.rint(triplet * 128 + 128).astype(np.uint8).flatten()\n",
    "    \n",
    "    return triplet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load pre-processed image cutout triplets: [epochal science image, reference image, ZOGY difference image]. The ZTF cutout images are centered on the event candidate and are of size 63x63 pixels (or smaller, if the event is detected near the CCD edge) at a plate scale of 1$\"$ per pixel. We perform independent $L^2$-normalization of the epochal science, reference, and difference cutouts and stack them to form 63x63x3 triplets that are input into the model. Smaller examples are accordingly padded using a constant pixel value of $10^{-9}$. See function `make_triplet` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use memory mapping as the file is relatively large (1 GB)\n",
    "triplets = np.load('data/triplets.norm.npy', mmap_mode='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visuals\n",
    "\n",
    "Plot a few triplet examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.random.randint(0, high=len(df), size=5)\n",
    "for ii in ind:\n",
    "    print(f'candid: {df.loc[ii, \"candid\"]}, label: {df.loc[ii, \"label\"]}')\n",
    "    fig = plt.figure(figsize=(8, 2), dpi=100)\n",
    "    triplet = triplets[ii, :]\n",
    "    ax = fig.add_subplot(131)\n",
    "    ax.axis('off')\n",
    "    ax.imshow(triplet[:, :, 0], origin='lower', cmap=plt.cm.bone)\n",
    "    ax2 = fig.add_subplot(132)\n",
    "    ax2.axis('off')\n",
    "    ax2.imshow(triplet[:, :, 1], origin='lower', cmap=plt.cm.bone)\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    ax3.axis('off')\n",
    "    ax3.imshow(triplet[:, :, 2], origin='lower', cmap=plt.cm.bone)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the dataset a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "color_wheel = {0: \"#dc3545\", \n",
    "               1: \"#28a745\"}\n",
    "colors = df[\"label\"].map(lambda x: color_wheel.get(x))\n",
    "\n",
    "columns = ['magpsf', 'fwhm', 'ndethist', 'scorr']\n",
    "\n",
    "axx = scatter_matrix(df.loc[df.label >= 0, columns], \n",
    "                     alpha=0.2,  diagonal='hist', ax=ax, grid=True, color=colors,\n",
    "                     hist_kwds={'color': 'darkblue', 'alpha': 0, 'bins': 50})\n",
    "\n",
    "for rc in range(len(columns)):\n",
    "    rc_y_max = 0\n",
    "    for group in color_wheel.keys():\n",
    "        y = df[df.label == group][columns[rc]]\n",
    "        hh = axx[rc][rc].hist(y, bins=50, alpha=0.5, color=color_wheel[group], density=1)\n",
    "#         print(np.min(hh[0]), np.max(hh[0]))\n",
    "        rc_y_max = max(rc_y_max, np.max(hh[0]))\n",
    "        axx[rc][rc].set_ylim([0, 1.1*rc_y_max])\n",
    "\n",
    "# scatter_matrix(df.loc[df.label == 0, ['magpsf', 'fwhm', 'ndethist']], \n",
    "#                alpha=0.2,  diagonal='hist', ax=ax,\n",
    "#                hist_kwds={'color': '#dc3545', 'alpha': 0.5, 'bins': 100}, color='#dc3545')\n",
    "# scatter_matrix(df.loc[df.label == 1, ['magpsf', 'fwhm', 'ndethist']], \n",
    "#                alpha=0.2,  diagonal='hist', ax=ax,\n",
    "#                hist_kwds={'color': '#28a745', 'alpha': 0.5, 'bins': 100}, color='#28a745')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['jd'].map(lambda x: Time(x, format='jd').datetime)\n",
    "fig = plt.figure(figsize=(7, 3), dpi=100)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(df.loc[df['label'] == 0, 'date'], bins=50, #linestyle='dashed',\n",
    "        color=color_wheel[0], histtype='step', label='bogus', linewidth=1.2)\n",
    "ax.hist(df.loc[df['label'] == 1, 'date'], bins=50,\n",
    "        color=color_wheel[1], histtype='step', label='real', linewidth=1.2)\n",
    "# ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Count')\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, linewidth=.3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 81\\% / 9\\% / 10\\% training/validation/test data split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "# set random seed for reproducable results:\n",
    "random_state = 42\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(triplets, df.label, \n",
    "                                                    test_size=test_split, random_state=random_state)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `braai` architecture\n",
    "\n",
    "We will use a simple custom VGG-like sequential model ($VGG6$; this architecture was first proposed by the Visual Geometry Group of the Department of Engineering Science, University of Oxford, UK). The model has six layers with trainable parameters: four convolutional and two fully-connected. The first two convolutional layers use 16 3x3 pixel filters each while in the second pair, 32 3x3 pixel filters are used. To prevent over-fitting, a dropout rate of 0.25 is applied after each max-pooling layer and a dropout rate of 0.5 is applied after the second fully-connected layer. ReLU activation functions (Rectified Linear Unit --  a function defined as the positive part of its argument) are used for all five hidden trainable layers; a sigmoid activation function is used for the output layer.\n",
    "\n",
    "![](img/fig-braai2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg6(input_shape=(63, 63, 3), n_classes: int = 1):\n",
    "    \"\"\"\n",
    "        VGG6\n",
    "    :param input_shape:\n",
    "    :param n_classes:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential(name='VGG6')\n",
    "    # input: 63x63 images with 3 channel -> (63, 63, 3) tensors.\n",
    "    # this applies 16 convolution filters of size 3x3 each.\n",
    "    model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape, name='conv1'))\n",
    "    model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', name='conv2'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', name='conv3'))\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', name='conv4'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(4, 4)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu', name='fc_1'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    # output layer\n",
    "    activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "    model.add(tf.keras.layers.Dense(n_classes, activation=activation, name='fc_out'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "`braai` is implemented using `TensorFlow` software and its high-level `Keras` API. We will use the binary cross-entropy loss function, the Adam optimizer, a batch size of 64, and a 81\\%/9\\%/10\\% training/validation/test data split. The training image data are weighted per class to mitigate the real vs. bogus imbalance in the data sets. To augment the data, the images may be flipped horizontally and/or vertically at random. No random rotations and translations will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report(path: str = './', stamp: str = None, report: dict = dict()):\n",
    "    f_name = os.path.join(path, f'report.{stamp}.json')\n",
    "    with open(f_name, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "# make train and test masks:\n",
    "_, _, mask_train, mask_test = train_test_split(df.label, list(range(len(df.label))),\n",
    "                                                   test_size=test_split, random_state=random_state)\n",
    "masks = {'training': mask_train, 'test': mask_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "optimizer = 'adam'\n",
    "epochs = 100\n",
    "patience = 50\n",
    "# epochs = 10\n",
    "# patience = 5\n",
    "validation_split = 0.1\n",
    "class_weight = True\n",
    "batch_size = 64\n",
    "\n",
    "# halt training if no gain in validation accuracy over patience epochs\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "data_augmentation = {'horizontal_flip': True,\n",
    "                     'vertical_flip': True,\n",
    "                     'rotation_range': 0,\n",
    "                     'fill_mode': 'constant',\n",
    "                     'cval': 1e-9}\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=data_augmentation['horizontal_flip'],\n",
    "                                                          vertical_flip=data_augmentation['vertical_flip'],\n",
    "                                                          rotation_range=data_augmentation['rotation_range'],\n",
    "                                                          fill_mode=data_augmentation['fill_mode'],\n",
    "                                                          cval=data_augmentation['cval'],\n",
    "                                                          validation_split=validation_split)\n",
    "\n",
    "training_generator = datagen.flow(x_train, y_train, batch_size=batch_size, subset='training')\n",
    "validation_generator = datagen.flow(x_train, y_train, batch_size=batch_size, subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: (63, 63, 3)\n"
     ]
    }
   ],
   "source": [
    "binary_classification = True if loss == 'binary_crossentropy' else False\n",
    "n_classes = 1 if binary_classification else 2\n",
    "\n",
    "# training data weights\n",
    "if class_weight:\n",
    "    # weight data class depending on number of examples?\n",
    "    if not binary_classification:\n",
    "        num_training_examples_per_class = np.sum(y_train, axis=0)\n",
    "    else:\n",
    "        num_training_examples_per_class = np.array([len(y_train) - np.sum(y_train), np.sum(y_train)])\n",
    "\n",
    "    assert 0 not in num_training_examples_per_class, 'found class without any examples!'\n",
    "\n",
    "    # fewer examples -- larger weight\n",
    "    weights = (1 / num_training_examples_per_class) / np.linalg.norm((1 / num_training_examples_per_class))\n",
    "    normalized_weight = weights / np.max(weights)\n",
    "\n",
    "    class_weight = {i: w for i, w in enumerate(normalized_weight)}\n",
    "\n",
    "else:\n",
    "    class_weight = {i: 1 for i in range(2)}\n",
    "    \n",
    "# image shape:\n",
    "image_shape = x_train.shape[1:]\n",
    "print('Input image shape:', image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg6(input_shape=image_shape, n_classes=n_classes)\n",
    "\n",
    "# set up optimizer:\n",
    "if optimizer == 'adam':\n",
    "    optimzr = tf.keras.optimizers.Adam(lr=3e-4, beta_1=0.9, beta_2=0.999,\n",
    "                                       epsilon=None, decay=0.0, amsgrad=False)\n",
    "elif optimizer == 'sgd':\n",
    "    optimzr = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True)\n",
    "else:\n",
    "    print('Could not recognize optimizer, using Adam')\n",
    "    optimzr = tf.keras.optimizers.Adam(lr=3e-4, beta_1=0.9, beta_2=0.999,\n",
    "                                       epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer=optimzr, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_t_stamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f'braai_{model.name}_{run_t_stamp}'\n",
    "\n",
    "h = model.fit_generator(training_generator,\n",
    "                        steps_per_epoch=len(x_train) // batch_size,\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=(len(x_train)*validation_split) // batch_size,\n",
    "                        class_weight=class_weight,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate the resulting model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluating on training set to check misclassified samples:')\n",
    "labels_training_pred = model.predict(x_train, batch_size=batch_size, verbose=1)\n",
    "# XOR will show misclassified samples\n",
    "misclassified_train_mask = np.array(list(map(int, df.label[masks['training']]))).flatten() ^ \\\n",
    "                           np.array(list(map(int, np.rint(labels_training_pred)))).flatten()\n",
    "misclassified_train_mask = [ii for ii, mi in enumerate(misclassified_train_mask) if mi == 1]\n",
    "\n",
    "misclassifications_train = {int(c): [int(l), float(p)]\n",
    "                            for c, l, p in zip(df.candid.values[masks['training']][misclassified_train_mask],\n",
    "                                               df.label.values[masks['training']][misclassified_train_mask],\n",
    "                                               labels_training_pred[misclassified_train_mask])}\n",
    "# print(misclassifications_train)\n",
    "\n",
    "print('Evaluating on test set for loss and accuracy:')\n",
    "preds = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "test_loss = float(preds[0])\n",
    "test_accuracy = float(preds[1])\n",
    "print(\"Loss = \" + str(test_loss))\n",
    "print(\"Test Accuracy = \" + str(test_accuracy))\n",
    "\n",
    "print('Evaluating on training set to check misclassified samples:')\n",
    "preds = model.predict(x=x_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# XOR will show misclassified samples\n",
    "misclassified_test_mask = np.array(list(map(int, df.label[masks['test']]))).flatten() ^ \\\n",
    "                          np.array(list(map(int, np.rint(preds)))).flatten()\n",
    "misclassified_test_mask = [ii for ii, mi in enumerate(misclassified_test_mask) if mi == 1]\n",
    "\n",
    "misclassifications_test = {int(c): [int(l), float(p)]\n",
    "                           for c, l, p in zip(df.candid.values[masks['test']][misclassified_test_mask],\n",
    "                                              df.label.values[masks['test']][misclassified_test_mask],\n",
    "                                              preds[misclassified_test_mask])}\n",
    "\n",
    "# round probs to nearest int (0 or 1)\n",
    "labels_pred = np.rint(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot accuracy vs. epoch and Loss vs. epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot confusion matrices with score threshold=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot False Negative Rate vs False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training report in json format\n",
    "print('Generating report...')\n",
    "r = {'Run time stamp': run_t_stamp,\n",
    "     'Model name': model_name,\n",
    "     'Model trained': 'vgg6',\n",
    "     'Batch size': batch_size,\n",
    "     'Optimizer': optimizer,\n",
    "     'Requested number of train epochs': epochs,\n",
    "     'Early stopping after epochs': patience,\n",
    "     'Training+validation/test split': test_split,\n",
    "     'Training/validation split': validation_split,\n",
    "     'Weight training data by class': class_weight,\n",
    "     'Random state': random_state,\n",
    "     'Number of training examples': x_train.shape[0],\n",
    "     'Number of test examples': x_test.shape[0],\n",
    "     'X_train shape': x_train.shape,\n",
    "     'Y_train shape': y_train.shape,\n",
    "     'X_test shape': x_test.shape,\n",
    "     'Y_test shape': y_test.shape,\n",
    "     'Data augmentation': data_augmentation,\n",
    "     'Test loss': test_loss,\n",
    "     'Test accuracy': test_accuracy,\n",
    "     'Confusion matrix': confusion_matr.tolist(),\n",
    "     'Normalized confusion matrix': confusion_matr_normalized.tolist(),\n",
    "     'Misclassified test candids': list(misclassifications_test.keys()),\n",
    "     'Misclassified training candids': list(misclassifications_train.keys()),\n",
    "     'Test misclassifications': misclassifications_test,\n",
    "     'Training misclassifications': misclassifications_train,\n",
    "     'Training history': h.history\n",
    "     }\n",
    "for k in r['Training history'].keys():\n",
    "    r['Training history'][k] = np.array(r['Training history'][k]).tolist()\n",
    "\n",
    "# print(r)\n",
    "\n",
    "save_report(path='./', stamp=run_t_stamp, report=r)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triplet(tr):\n",
    "    fig = plt.figure(figsize=(8, 2), dpi=100)\n",
    "    ax = fig.add_subplot(131)\n",
    "    ax.axis('off')\n",
    "    ax.imshow(tr[:, :, 0], origin='upper', cmap=plt.cm.bone)\n",
    "    ax2 = fig.add_subplot(132)\n",
    "    ax2.axis('off')\n",
    "    ax2.imshow(tr[:, :, 1], origin='upper', cmap=plt.cm.bone)\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    ax3.axis('off')\n",
    "    ax3.imshow(tr[:, :, 2], origin='upper', cmap=plt.cm.bone)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -O 714287740515015072.json https://raw.githubusercontent.com/dmitryduev/braai/master/nb/714287740515015072.json\n",
    "#!wget -O 893215910715010007.json https://raw.githubusercontent.com/dmitryduev/braai/master/nb/893215910715010007.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('714287740515015072.json', 'r') as f:\n",
    "    al = json.load(f)\n",
    "print(al['candidate']['rb'])\n",
    "tr = make_triplet(al)\n",
    "plot_triplet(tr)\n",
    "model.predict(np.expand_dims(tr, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('893215910715010007.json', 'r') as f:\n",
    "    al = json.load(f)\n",
    "print(al['candidate']['rb'])\n",
    "tr = make_triplet(al)\n",
    "plot_triplet(tr)\n",
    "model.predict(np.expand_dims(tr, axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
