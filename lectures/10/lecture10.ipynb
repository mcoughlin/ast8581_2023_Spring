{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from IPython.lib.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6 (Monday), AST 8581 / PHYS 8581 / CSCI 8581: Big Data in Astrophysics\n",
    "\n",
    "### Michael Coughlin <cough052@umn.edu>, Jie Ding <dingj@umn.edu>\n",
    "\n",
    "\n",
    "With contributions totally ripped off from Gautham Narayan (UIUC), Michael Steinbach (UMN), and Nico Adams (UMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 4 Takeaways\n",
    "    \n",
    "This week seemed to take a similar amount of time. Some more confusion with the introduction of magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW Survey Takeaways\n",
    "\n",
    "*  About 75% of the class reported less than 6 hours for HW4, and most of the rest 6-9 hours, so reasonably similar to last week.\n",
    "\n",
    "* There were a few comments on how the notebook fits in the broader context of the plan for the class. I will try to remember at the start of each day to provide this context.\n",
    "\n",
    "* There was some commentary on the Jupyter notebook completeness. If there are missing / confusing pieces, please do let us know.\n",
    "\n",
    "### Reminder\n",
    "\n",
    "* Please keep thinking about what kind of project you might be interested in. The form is here: https://forms.gle/MtVcucLKReKJby1N6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where do we stand?\n",
    "\n",
    "Foundations of Data and Probability -> Statistical frameworks (Frequentist vs Bayesian) -> Estimating underlying distributions -> Analysis of Time series (periodicity) -> Analysis of Time series (variability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Last Class: Lomb Scargle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pros:\n",
    "- Works with unevenly sampled data, and binning in phase helps build statistics even with noisy data\n",
    "\n",
    "Cons:\n",
    "- ANY PERIODIC SIGNAL IN THE DATA WILL EXHIBIT SOME POWER\n",
    "    - This includes aliases of the true period\n",
    "- Multiband extension requires  that the period/frequency is the same across all channels/passbands \n",
    "    - this is not the case for many time-series phenomena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today: How do check if the object is even variable?\n",
    "\n",
    "When dealing with time series data, the first thing that we want to know is if the system that we are studying is even variable (otherwise, there is no point doing time series analysis!).  \n",
    "\n",
    "In the context of frequentist statistics, this is a question of whether our data would have been obtained by chance if the no-variability null hypothesis were correct.\n",
    "\n",
    "If we find that our source *is* variable, then our time-series analysis has two main goals:\n",
    "1. Characterize the temporal correlation between different values of $y$ (i.e., characterize the \"light curve\").  For example by learning the parameters for a model.\n",
    "2. Predict future values of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If the errors are known and Gaussian, we can simply compute $\\chi^2$ and the corresponding $p$ values for variation in a signal.\n",
    "\n",
    "For a sinusoidal variable signal\n",
    "\n",
    "# $$y(t) = A \\sin(\\omega t)$$\n",
    "\n",
    "with constant errors, $\\sigma$, then variance is \n",
    "\n",
    "# $$V = \\sigma^2 + A^2/2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# If $A=0$ (no variability)\n",
    "* ### $\\chi^2_{\\rm dof}=\\frac{1}{N} \\sum_j \\left(\\frac{y_j}{\\sigma}\\right)^2 \\sim V/\\sigma^2$\n",
    "* ### $\\chi^2_{\\rm dof}$ has  expectation value of 1 and std dev  of $\\sqrt\\frac{2}{N}$\n",
    "\n",
    "# If $|A|>0$ (variability)\n",
    "* ### $\\chi^2_{\\rm dof}$ will be larger\n",
    "than 1. \n",
    "* ### probability that $\\chi^2_{\\rm dof}>1 + 3 \\sqrt{2/N}$  is about 1 in 1000 (i.e., $>3\\sigma$ above 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If this **false-positive rate** is acceptable (even without variability, 1 times in 1000 we will observer a $\\chi^2_{\\rm dof}$ above this threshold) then the **minimum detectable amplitude** is:\n",
    "\n",
    "### $$A > \\left( \\frac{72}{N} \\right)^\\frac{1}{4} \\sigma \\approx 2.9 \\sigma N^{-{1 \\over 4}}$$\n",
    "\n",
    "which follows from equating $V/\\sigma^2=1 + 3 \\sqrt{2/N}$, expanding $V$, and solving for $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It takes $N > 70$ to reach $A=\\sigma$.\n",
    "\n",
    "For $N=100$ data points, the minimum detectable amplitude is $A=0.92\\sigma$\n",
    "\n",
    "For $N=1000$, $A = 0.52\\sigma$ \n",
    "\n",
    "That is, **if we have enough observations, we can actually detect variability whose amplitude is smaller than the uncertainty in the measurements.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that this is the best that we can do under the assumption of the null hypothesis of no variability.  \n",
    "\n",
    "If instead we know the model (not limited to periodic variability), then we can perform a [**\"matched filter\"**](https://en.wikipedia.org/wiki/Matched_filter) analysis and improve upon this (i.e., we can positively identify lower-amplitude variability).  \n",
    "\n",
    "**Indeed in a Bayesian analysis, we must specify a model.**\n",
    "\n",
    "For non-periodic variability, the system can either be **stochastic** (like the stock market) or **temporally localized** (such as a flare/burst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameter Estimation, Model Selection, and Classification\n",
    "\n",
    "Time series analysis can be conducted in either the time domain or the frequency domain. We'll use all the same machinery we worked with in the first half of class. \n",
    "\n",
    "We can fit a model to $N$ data points $(t_i,y_i)$:\n",
    "\n",
    "# $$y_i(t_i) = \\sum_{m=1}^M \\beta_m T_m(t_i|\\theta_m) + \\epsilon_i,$$\n",
    "\n",
    "where the functions, $T_m$, do not need to be periodic, $t_i$ does not need to be evenly sampled and $\\theta_m$ are the model parameters.\n",
    "\n",
    "So, for example, if we have\n",
    "\n",
    "# $$y_i(t_i) = a \\sin(\\omega_0 t_i) + b \\cos (\\omega_1 t_i),$$\n",
    "\n",
    "then $a=\\beta_0$, $b=\\beta_1$, $\\omega_0=\\theta_0$, and $\\omega_1 = \\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Determining if you have the right model can also use the same machinery as we've used (posterior predictive checks, information criterion):\n",
    "\n",
    "Common deterministic models include\n",
    "\n",
    "# $$T(t) = \\sin(\\omega t)$$\n",
    "\n",
    "and\n",
    "\n",
    "# $$T(t) = \\exp(-\\alpha t),$$\n",
    "\n",
    "where the frequency, $\\omega$, and decay rate, $\\alpha$, are parameters to be estimated from the data.\n",
    "\n",
    "You might also consider a \"chirp\" signal with\n",
    "\n",
    "# $$T(t) = \\sin(\\phi + \\omega t + \\alpha t^2).$$\n",
    "\n",
    "(another way of thinking of a chirp is that the *frequency varies with time*; $\\omega_{\\rm instantaneous} = \\omega + \\alpha t$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temporally Localized Signals ($\\S$ 10.4)\n",
    "\n",
    "Let's begin with the case of a stationary signal with an event localized in time.\n",
    "An example would be the signature of a [gravitational wave from LIGO](https://www.ligo.caltech.edu/news/ligo20160615).\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "            <td><img src=\"figures/small_multiples.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this case we know the expected shape of the signal and the noise properties are understood, so we can do what is called *forward modeling*.  Specifically, we will identify the signal by using a **matched filter** (with MCMC to search for the parameter covariances).\n",
    "\n",
    "Even if we didn't know the shape of the distribution, we could use a non-parametric form to perform matched filter analysis.  Furthermore, for complex signals we can marginalize over \"nuisance\" parameters (e.g. start time or phase) that are not important for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling a time-series when we have a forward model\n",
    "\n",
    "### In the first half of this class, we were able to write down a nice parametric model for our observations\n",
    "\n",
    "### However, precisely because time is an exogenous variable - just a counter that we happen to record along with our measurements - we can't usually do that anymore. \n",
    "\n",
    "### A notable exception is for periodic phenomena, where while the time stamp is not important, there are characteristic time-scale that we can use to build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The best example of this is radial velocity experiments used to detect exoplanets\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "            <td><img src=\"figures/Radial-Velocity-Method-star-orbits.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Here, the behavior of the system is governed by Kepler's laws and there is a honest-to-goodness parametric model to describe the observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### You can write down simple models to describe the milk production of cows, and while it is true that any cows you launch will also obey Kepler's laws, there's no fundamental physics that governs their milk production (which is likely to cease in short order if you do actually test bovine adherence to Kepler's laws). \n",
    "\n",
    "### While the radial velocity method is expensive, because we can build very high resolution spectrographs (remember these are just devices that are mapping energy differences into angular separation), they're likely our best hope for finding an Earth-like exoplanet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Class Exercise - Looking for planets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn; seaborn.set() #nice plot formatting\n",
    "import pandas as pd\n",
    "import corner\n",
    "from collections import namedtuple\n",
    "from scipy import optimize\n",
    "from gatspy.periodic import LombScargleFast\n",
    "import emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "data = pd.read_csv('data/47UrsaeMajoris.txt', delim_whitespace=True)\n",
    "t, rv, rv_err = data.values.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "Visualize this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Model\n",
    "\n",
    "The first important step is to define a mathematical (and computational) model of how the parameters of interest are reflected in our observations.\n",
    "\n",
    "Some references relating to what we're going to compute below:\n",
    "\n",
    "- Balan 2009: http://adsabs.harvard.edu/abs/2009MNRAS.394.1936B\n",
    "- Exofit Manual: http://www.star.ucl.ac.uk/~lahav/ExoFitv2.pdf\n",
    "- Hou 2014: http://arxiv.org/pdf/1401.6128.pdf\n",
    "\n",
    "The equation for radial velocity is this:\n",
    "\n",
    "$$\n",
    "v(t) = V - K[ \\sin(f + \\omega) + e \\sin(\\omega)]\n",
    "$$\n",
    "\n",
    "where $V$ is the overall velocity of the system, and\n",
    "\n",
    "$$\n",
    "K = \\frac{m_p}{m_s + m_p} \\frac{2\\pi}{T}\\frac{a \\sin i}{\\sqrt{1 - e^2}}\n",
    "$$\n",
    "\n",
    "The true anomaly $f$ satisfies\n",
    "\n",
    "$$\n",
    "\\cos(f) = \\frac{\\cos(E) - e}{1 - e\\cos E}\n",
    "$$\n",
    "\n",
    "Rearranging this we can write\n",
    "$$\n",
    "f = 2 \\cdot{\\rm atan2}\\left(\\sqrt{1 + e}\\sin(E/2), \\sqrt{1 - e} \\cos(E/2)\\right)\n",
    "$$\n",
    "\n",
    "The eccentric anomaly $E$ satisfies\n",
    "$$\n",
    "M = E - e\\sin E\n",
    "$$\n",
    "\n",
    "and the mean anomaly is\n",
    "$$\n",
    "M = \\frac{2\\pi}{T}(t + \\tau)\n",
    "$$\n",
    "\n",
    "and $\\tau$ is the time of pericenter passage, which we'll parametrize with the parameter $\\chi = \\tau /  T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These are the parameters needed to compute the radial velocity:\n",
    "\n",
    "- $T$: orbital period\n",
    "- $K$: amplitude of RV oscillation\n",
    "- $V$: secular offset of RV oscillation\n",
    "- $e$: eccentricity\n",
    "- $\\omega$: longitude of periastron\n",
    "- $\\chi$: dimensionless phase offset\n",
    "\n",
    "Additionally, we will fit a scatter parameter $s$ which accounts for global data errors not reflected in the reported uncertainties (this is very similar to the third parameter from the linear fit we saw earlier)\n",
    "\n",
    "For convenience, we'll store these parameters in a ``namedtuple``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "params = namedtuple('params', ['T', 'e', 'K', 'V', 'omega', 'chi', 's'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### These just define the model we wrote down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "\n",
    "@np.vectorize\n",
    "def compute_E(M, e):\n",
    "    \"\"\"Solve Kepler's eqns for eccentric anomaly given mean anomaly\"\"\"\n",
    "    f = lambda E, M=M, e=e: E - e * np.sin(E) - M\n",
    "    return optimize.brentq(f, 0, 2 * np.pi)\n",
    "\n",
    "\n",
    "def radial_velocity(t, theta):\n",
    "    \"\"\"Compute radial velocity given orbital parameters\"\"\"\n",
    "    T, e, K, V, omega, chi = theta[:6]\n",
    "    \n",
    "    # compute mean anomaly (0 <= M < 2pi)\n",
    "    M = ...\n",
    "    \n",
    "    # solve for eccentric anomaly\n",
    "    E = compute_E(M, e)\n",
    "    \n",
    "    # compute true anomaly\n",
    "    f = ...\n",
    "    \n",
    "    # compute radial velocity\n",
    "    return V - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Likelihood and Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "\n",
    "theta_lim = params(T=(0.2, 2000),\n",
    "                   e=(0, 1),\n",
    "                   K=(0.01, 2000),\n",
    "                   V=(-2000, 2000),\n",
    "                   omega=(0, 2 * np.pi),\n",
    "                   chi=(0, 1),\n",
    "                   s=(0.001, 100))\n",
    "theta_min, theta_max = map(np.array, zip(*theta_lim))\n",
    "\n",
    "def log_prior(theta):\n",
    "    if np.any(theta < theta_min) or np.any(theta > theta_max):\n",
    "        return -np.inf # log(0)\n",
    "    \n",
    "    # Jeffreys Prior on T, K, and s\n",
    "    return -np.sum(np.log(theta[[0, 2, 6]]))\n",
    "\n",
    "def log_likelihood(theta, t, rv, rv_err):\n",
    "    sq_err = rv_err ** 2 + theta[6] ** 2\n",
    "    rv_model = radial_velocity(t, theta)\n",
    "    return -0.5 * np.sum(np.log(sq_err) + (rv - rv_model) ** 2 / sq_err)\n",
    "\n",
    "def log_posterior(theta, t, rv, rv_err):\n",
    "    ln_prior = log_prior(theta)\n",
    "    if np.isinf(ln_prior):\n",
    "        return ln_prior\n",
    "    else:\n",
    "        return ln_prior + log_likelihood(theta, t, rv, rv_err)\n",
    "    \n",
    "def make_starting_guess(t, rv, rv_err):\n",
    "    model = LombScargleFast()\n",
    "    model.optimizer.set(period_range=theta_lim.T,\n",
    "                        quiet=True)\n",
    "    model.fit(t, rv, rv_err)\n",
    "\n",
    "    rv_range = 0.5 * (np.max(rv) - np.min(rv))\n",
    "    rv_center = np.mean(rv)\n",
    "    return params(T=model.best_period,\n",
    "                  e=0.1,\n",
    "                  K=rv_range,\n",
    "                  V=rv_center,\n",
    "                  omega=np.pi,\n",
    "                  chi=0.5,\n",
    "                  s=rv_err.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Now use my initial guess and sample the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "theta_guess = make_starting_guess(t, rv, rv_err)\n",
    "theta_guess\n",
    "\n",
    "ndim = len(theta_guess)  # number of parameters in the model\n",
    "nwalkers = 50  # number of MCMC walkers\n",
    "\n",
    "# start with a tight distribution of theta around the initial guess\n",
    "rng = np.random.RandomState(42)\n",
    "starting_guesses = theta_guess * (1 + 0.1 * rng.randn(nwalkers, ndim))\n",
    "\n",
    "# RUN MCMC - YOUR CODE HERE\n",
    "sampler = emcee.EnsembleSampler(...\n",
    "pos, prob, state = sampler.run_mcmc(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a little function to plot the chains\n",
    "def plot_chains(sampler):\n",
    "    fig, ax = plt.subplots(ndim, figsize=(8, 10), sharex=True)\n",
    "    for i in range(ndim):\n",
    "        ax[i].plot(sampler.chain[:, :, i].T, '-k', alpha=0.2);\n",
    "        ax[i].set_ylabel(params._fields[i])\n",
    "\n",
    "        \n",
    "# PLOT YOUR CHAINS - YOUR CODE HERE\n",
    "plot_chains(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we've seen before the initial steps in the chain aren't useful samples and it takes a while to reach a stationary distribution. Notice though that the chains are *NOT* all mixed - there are some that are wandering away from the the remaining traces, and being stubborn about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN A FULL MCMC HERE RESETING THE SAMPLER AND RESTARTING FROM THE LAST POSITION AFTER THE BURN-IN\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT YOUR CHAINS AGAIN - YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND LOOK AT THE PERIOD AND ECCENTRICITY \n",
    "# WE CAN'T GET MASSES FROM THE RADIAL VELOCITY METHOD, \n",
    "# BUT IF WE CARE ABOUT HABITABILITY, LOOKING AT e IS A GOOD IDEA\n",
    "# Use corner.py or similar to look at the histograms\n",
    "corner.corner(sampler.flatchain[:, :2], labels=params._fields[:2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS TO VISUALIZE THE MODEL AGREEMENT WITH THE DATA\n",
    "t_fit = np.linspace(t.min(), t.max(), 1000)\n",
    "rv_fit = [radial_velocity(t_fit, sampler.flatchain[i])\n",
    "          for i in rng.choice(sampler.flatchain.shape[0], 200)]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.errorbar(t, rv, rv_err, fmt='.k')\n",
    "plt.plot(t_fit, np.transpose(rv_fit), '-k', alpha=0.01)\n",
    "plt.xlabel('time (days)')\n",
    "plt.ylabel('radial velocity (km/s)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS TO GET A SENSE OF THE PERIOD AND ECCENTRICITY\n",
    "mean = sampler.flatchain.mean(0)\n",
    "std = sampler.flatchain.std(0)\n",
    "\n",
    "print(\"Period       = {0:.0f} +/- {1:.0f} days\".format(mean[0], std[0]))\n",
    "print(\"eccentricity = {0:.2f} +/- {1:.2f}\".format(mean[1], std[1]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
